[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mohammad Sayeef Alam",
    "section": "",
    "text": "I am a doctoral candidate at the Norwegian University of Science and Technology in Trondheim, with a strong background in health data analysis. My research focuses on statistical genetics and gastrointestinal diseases, where I work with tools like R, Python, and Bash.\nIn my spare time, I enjoy creating visualizations and building interactive applications to support decision-making in healthcare, and always curious how to use programming and real-world data to solve problems in healthcare.\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I am a final-year PhD candidate in Genetic Epidemiology at the Norwegian University of Science and Technology (NTNU) in Trondheim. My research focuses on statistical genetics and gastrointestinal diseases, where I analyze high-dimensional genetic data and link it with real-world health records to uncover differences across population groups.\nI hold dual master’s degrees in Biostatistics and a bachelor’s degree in Statistics. Over the past six years, I have gained experience working with diverse health-related datasets, applying a range of statistical methods in epidemiology, biostatistics, clinical trials, and genetics.\nI have hands-on experience with tools such as R, STATA, SAS, Excel (including VBA), and Bash. I particularly enjoy building interactive applications using Shiny to support data-driven decision-making. I am technically minded and strive to communicate complex ideas in a way that is accessible to different audiences. I value a positive working environment and am always open to new challenges. I am fluent in English and still learning Norwegian.\nOutside of research, I enjoy hiking, skiing, and watching anime. I’m also learning to swim. Community and connection are important to me—I volunteer at the MSiT mosque in Trondheim during Ramadan, helping serve meals to people of all backgrounds.\nMy career goal is to contribute to evidence synthesis and informed decision-making in healthcare through rigorous research, thoughtful analytics, and collaborative science that benefits communities worldwide.\nSee ya later ⛷️\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs.html",
    "href": "blogs.html",
    "title": "MSA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMonte Carlo Markov Chain (MCMC)\n\n\n\nDecision modelling\n\n\nMethods\n\n\nR\n\n\n\nA short description\n\n\n\nAug 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Economics and R\n\n\n\nMethods\n\n\nR\n\n\n\nA short description\n\n\n\nJul 21, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLearnings from first R package development\n\n\n\nR\n\n\nProgramming\n\n\n\nBasic checks and debugging while R package development\n\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComplete Git and SSH Setup Guide: From Zero to Push\n\n\n\nTerminal\n\n\nProgramming\n\n\n\nThis guide provides step-by-step instructions to connect git, clone repo and push commits\n\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Missing to Meaningful: Beginner’s Guide to Multiple Imputation\n\n\n\nR\n\n\nProgramming\n\n\n\nA beginner-friendly guide to handling missing data using Multiple Imputation with the mice package in R.\n\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPolygenic Risk Score\n\n\n\nMethods\n\n\nR\n\n\n\nA guide to understanding, implementing, and evaluating polygenic risk scores using both classical and modern methods in R.\n\n\n\nJun 21, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBeyond QC: A Practical Guide to GWAS with SAIGE\n\n\n\nGenetics\n\n\nMethods\n\n\nBash\n\n\nR\n\n\n\nA hands-on guide for performing GWAS after preliminary QC, covering phenotype preparation, and association testing using SAIGE.\n\n\n\nApr 21, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "workshops.html",
    "href": "workshops.html",
    "title": "Certifications",
    "section": "",
    "text": "2025\n\n  \n\n\n\n2024\n\n  \n  \n\n\n  \n  \n\n\n\n2021\n\n  \n\n\n\n2020\n\n\n   \n\n  \n\n\n  \n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "Download PDF\nLocation: Trondheim, Norway\nEmail: alam.sayeef@gmail.com\nPhone: +47 4867 1602\nLinkedIn: linkedin.com/in/mohasal"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "MSA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 10, 2024\n\n\nAssessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.\n\n\nMohammad Sayeef Alam, Rebecka Hjort, Kristian Hveem, Knut E. A. Lundin, Iris H. Jonkers, Ludvig M. Sollid, Eivind Ness-Jensen\n\n\n\n\nSep 9, 2024\n\n\nThe IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.\n\n\nMohammad Sayeef Alam, Laurent F Thomas, Ben Brumpton, Kristian Hveem, Knut E A Lundin, Sebo Withoff, Iris H Jonkers, Ludvig M Sollid, Rebecka Hjort, Eivind Ness Jensen\n\n\n\n\nSep 6, 2024\n\n\nAssessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.\n\n\nMohammad Sayeef Alam, Laurent F Thomas, Ben Brumpton, Kristian Hveem, Knut E A Lundin, Sebo Withoff, Iris H Jonkers, Ludvig M Sollid, Rebecka Hjort, Eivind Ness Jensen\n\n\n\n\nSep 5, 2024\n\n\nThe IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.\n\n\nMohammad Sayeef Alam, Laurent F Thomas, Ben Brumpton, Kristian Hveem, Knut E A Lundin, Sebo Withoff, Iris H Jonkers, Ludvig M Sollid, Rebecka Hjort, Eivind Ness Jensen\n\n\n\n\nNov 4, 2019\n\n\nPrevalence of communicable diseases and its epidemiological correlates in major urban populated states of India: Evidence from a nationally representative sample\n\n\nMohammad Sayeef Alam, Balram Paswan, Mohammed Illias Sheikh\n\n\n\n\nDec 18, 2018\n\n\nShifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study\n\n\nMohammad Sayeef Alam, Md Illias Kanchan Sk\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "MSA",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nPackaging MOON in R\n\n\nFrom Excel to R - Advancing Decision Analytics for Cost-Effective Health Interventions for Obesity\n\n\n\nR\n\n\nExcel\n\n\n\n\n\n\n\n\n\nMohammad Sayeef Alam, Gudrun Maria Waaler Bjørnelv\n\n\n\n\n\n\n\n\n\n\n\n\nQALY Shortfall calculator for Nordics\n\n\nAn RShiny app to estimate the absolute and proportion shortfall in QALY for the five Nordic countries\n\n\n\nR\n\n\nShiny\n\n\n\n\n\n\n\n\n\nMohammad Sayeef Alam, Gudrun Maria Waaler Bjørnelv, Christina Hansen Edwards, Yvonne Anne Michel, Jon Magnussen\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "resume.html#associate-parexel-remote-india",
    "href": "resume.html#associate-parexel-remote-india",
    "title": "Curriculum Vitae",
    "section": "Associate, Parexel – Remote, India",
    "text": "Associate, Parexel – Remote, India\nJuly 2021 – February 2022\n\nDeveloped interactive platforms (e.g., RShiny) for economic evaluations.\nPerformed statistical analyses aligned with HTA guidelines across multiple disease areas.\nAutomated dashboards (Excel/VBA) for clinical endpoint visualization.\nCoordinated projects and managed client expectations.\nContributed to SAPs and validated datasets.\nPresented analytical results to stakeholders and refined deliverables."
  },
  {
    "objectID": "resume.html#phd-ntnu-trondheim-norway",
    "href": "resume.html#phd-ntnu-trondheim-norway",
    "title": "Curriculum Vitae",
    "section": "PhD, NTNU – Trondheim, Norway",
    "text": "PhD, NTNU – Trondheim, Norway\nApril 2022 – Current\n\nApplied advanced statistical methods to genetic and registry data.\nDesigned analysis pipelines to improve efficiency.\nPresented findings at conferences and published in peer-reviewed journals.\nCollaborated with external research groups.\nBuilt academic networks and secured over 150,000 NOK in funding."
  },
  {
    "objectID": "resume.html#nordic-shortfall-calculator",
    "href": "resume.html#nordic-shortfall-calculator",
    "title": "Curriculum Vitae",
    "section": "Nordic Shortfall Calculator",
    "text": "Nordic Shortfall Calculator\n\nDeveloped a model to calculate shortfall for Denmark, Finland, Norway, and Sweden.\nProvides estimates of QALE based on EQ-5D/3D values.\nIncludes comparative options, discount rates, and dynamic inputs."
  },
  {
    "objectID": "projects.html#global-biodiversity-shiny-dashboard-for-poland",
    "href": "projects.html#global-biodiversity-shiny-dashboard-for-poland",
    "title": "Mohammad Sayeef Alam",
    "section": "",
    "text": "The GBIF Data Explorer is a web application that allows users to explore and analyze biodiversity data for Poland from the Global Biodiversity Information Facility (GBIF) database.\nThe app is built using the Shiny package for R. It includes several visualizations and filters created using the following packages: DT, shiny, shinyWidgets, shinythemes, shinyjs, bslib, bsicons, lubridate, leaflet, highcharter…. Check out the shiny dashboard"
  },
  {
    "objectID": "projects.html#collaborating-between-python-and-r-using-reticulate",
    "href": "projects.html#collaborating-between-python-and-r-using-reticulate",
    "title": "Mohammad Sayeef Alam",
    "section": "Collaborating between Python and R using Reticulate",
    "text": "Collaborating between Python and R using Reticulate\n\n\n\n\n\n\nBoth R and Python are powerful data science languages and can be used to accomplish end-to-end analytics projects. Reticulate makes it easy to collaborate and talk between the two languages. I think of it as a translator on my fingertips.\nMy teammate and I participated in Microsoft hackathon where we accessed data from APIs using Python, pulled the output in R using Reticulate and created visualizations to derive business insights using Ggplot2… Check out how"
  },
  {
    "objectID": "projects.html#data-visualizations---animation-and-interactivity-using-gganimate-and-plotly",
    "href": "projects.html#data-visualizations---animation-and-interactivity-using-gganimate-and-plotly",
    "title": "Mohammad Sayeef Alam",
    "section": "Data visualizations - Animation and Interactivity using Gganimate and Plotly",
    "text": "Data visualizations - Animation and Interactivity using Gganimate and Plotly\n\n\nAnimation brings in a key component of analysis where the third parameter you want to animate is not directly visible in your plot. Sometimes it can tell a powerful story and sometimes it can just add some flair to an otherwise low-key story.\nThis blog goes through examples and code to show how easy it is to use gganimate and plotly packages to animate your data visualizations and make them interactive…. Continue reading"
  },
  {
    "objectID": "projects.html#sales-forecasting-and-anomaly-detection-shiny-dashboard",
    "href": "projects.html#sales-forecasting-and-anomaly-detection-shiny-dashboard",
    "title": "Mohammad Sayeef Alam",
    "section": "Sales forecasting and anomaly detection Shiny dashboard",
    "text": "Sales forecasting and anomaly detection Shiny dashboard\n\n\n\n\n\n\nThis dashboard, built with Shiny Flex capability, looks at data from EverythingYouWillNeed.com website (fake data) to report out odaily product sales. You can select the product, the dates to analyze and click on apply to replicate the report, check sales forecast and anomalies for the specific product choice.\nGiving your stakeholders the ability to play with product grain forecast and anomaly detection is a great was to get them on-board prior to deploying any model at scale. This Shiny dashboard was built specifically for that purpose…. Link to play with the Shiny dashboard"
  },
  {
    "objectID": "projects.html#qaly-shortfall-calculator-for-nordics",
    "href": "projects.html#qaly-shortfall-calculator-for-nordics",
    "title": "MSA",
    "section": "",
    "text": "The Nordic Shortfall Calculator is a web application that allows users to explore and analyze the QALY shortfall based on updated utility values as well as compare them to previous values or even values used by other Nordic countries.\nThe app is built using the Shiny package for R. It includes several visualizations and filters created using the following packages: DT, shiny, shinyWidgets, shinythemes, shinyjs, bslib, bsicons, lubridate, leaflet, highcharter…. Check out the shiny app"
  },
  {
    "objectID": "projects.html#packaging-moon",
    "href": "projects.html#packaging-moon",
    "title": "MSA",
    "section": "",
    "text": "Both R and Excel are powerful data science tools and can be used to accomplish end-to-end analytics projects.\nGudrun and I, are trying to move the decision analytics model built on excel to R, to promote open-access science. The decision analytics model runs a Markov model to assess the cost effectiveness of intervention for obesity."
  },
  {
    "objectID": "talks.html#lab-reports",
    "href": "talks.html#lab-reports",
    "title": "Team Documents",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "talks.html#meeting-notes",
    "href": "talks.html#meeting-notes",
    "title": "Team Documents",
    "section": "Meeting Notes",
    "text": "Meeting Notes\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "Mohammad Sayeef Alam",
    "section": "",
    "text": "I’m a doctoral candidate at the Norwegian University of Science and Technology (NTNU) in Trondheim, where my research focuses on statistical genetics and gastrointestinal diseases. With a strong foundation in statistics and biostatistics, I work with real-world health data to support better decision-making in healthcare.\nMy work involves applying advanced statistical methods and programming in R, Python, and bash. I also enjoy building interactive tools—such as Shiny apps—that help make complex data more accessible and actionable. I’m especially motivated by the opportunity to use programming to address real-world challenges in healthcare.\nWhat motivates me most is the opportunity to apply programming and analytics to real-world challenges in healthcare. I find it rewarding to contribute to projects that have practical impact, whether through research, collaboration, or tool development.\nBeyond research, I value community and connection. During the month of Ramadan, I volunteer at the MSiT mosque in Trondheim, helping serve food to people of all faiths—whether they are breaking their fast or simply sharing a warm meal with friends. It’s a small way to contribute to a welcoming and inclusive environment.\nOutside of work, I enjoy hiking, swimming, improving my Norwegian and cooking for others. These activities help me stay grounded and bring balance to my academic life. I’m always eager to learn, collaborate, and contribute to projects that make a meaningful impact."
  },
  {
    "objectID": "blogs/rpackagedev.html",
    "href": "blogs/rpackagedev.html",
    "title": "Learnings from first R package development",
    "section": "",
    "text": "This documentation is for individuals such as me, to create more than basic R package while debugging common errors, warnings and notes."
  },
  {
    "objectID": "blogs/gitsetup.html",
    "href": "blogs/gitsetup.html",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "",
    "text": "Whether you’re a beginner developer or switching to a new machine, setting up Git with SSH authentication can seem daunting. This comprehensive guide will walk you through every step to get you from a fresh installation to pushing your first commit securely.\nBy the end of this guide, you’ll be able to: - Configure Git with your identity - Generate and set up SSH keys for secure authentication - Create and manage local Git repositories - Connect your local work to remote repositories (GitHub, GitLab, etc.) - Understand the basic Git workflow for daily development"
  },
  {
    "objectID": "blogs/rpackagedev.html#step-4.5-perform-checks",
    "href": "blogs/rpackagedev.html#step-4.5-perform-checks",
    "title": "Learnings from first R package development",
    "section": "Step 4.5: Perform checks",
    "text": "Step 4.5: Perform checks\n\nCodedevtools::document()\ndevtools::check()"
  },
  {
    "objectID": "blogs/rpackagedev.html#step-6.5-perform-checks",
    "href": "blogs/rpackagedev.html#step-6.5-perform-checks",
    "title": "Learnings from first R package development",
    "section": "Step 6.5: Perform checks",
    "text": "Step 6.5: Perform checks\n\nCodedevtools::document()\ndevtools::check()"
  },
  {
    "objectID": "projects/nsc.html",
    "href": "projects/nsc.html",
    "title": "QALY Shortfall calculator for Nordics",
    "section": "",
    "text": "The Nordic QALY Shortfall Calculator is an interactive RShiny application developed to support health technology assessments (HTAs) across Denmark, Finland, Iceland, Norway, and Sweden. It estimates Quality-Adjusted Life Year (QALY) shortfalls—a key metric for evaluating disease severity and guiding healthcare prioritization. By comparing the quality-adjusted life expectancy (QALE) of individuals with a specific disease to that of the general population, the tool provides transparent, evidence-based insights using national life tables and health-related quality of life (HRQoL) data.\nThis project transforms static health economic evaluations into a dynamic, user-driven experience. Built entirely in RShiny, the app integrates validated utility scoring algorithms (e.g., EQ-5D-3L/5L) and applies the Sullivan method to calculate QALE and shortfall metrics. Users can input demographic and clinical parameters, select utility scenarios, and instantly visualize results. The app’s modular design supports flexible scenario comparisons, enabling rapid exploration of severity-based modifiers and their impact on HTA decisions.\nBy making QALY shortfall estimation accessible and interactive, the Nordic QALY Shortfall Calculator empowers researchers, policymakers, and HTA professionals to apply severity-based weighting in a consistent and transparent manner. It facilitates cross-country comparisons, supports methodological harmonization, and encourages data-driven decision-making. The app’s open and adaptable framework also allows for future updates and extensions, making it a valuable resource for ongoing research and policy development in the Nordic region and beyond.\nThe app is built using the Shiny package for R. It includes several visualizations and filters created using the following packages: DT, shiny, shinyWidgets, shinythemes, shinyjs, bslib, bsicons, lubridate, leaflet, highcharter. Try the app🔗\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/nsc.html#qaly-shortfall-calculator-for-nordics",
    "href": "projects/nsc.html#qaly-shortfall-calculator-for-nordics",
    "title": "MSA",
    "section": "",
    "text": "The Nordic Shortfall Calculator is a web application that allows users to explore and analyze the QALY shortfall based on updated utility values as well as compare them to previous values or even values used by other Nordic countries.\nThe app is built using the Shiny package for R. It includes several visualizations and filters created using the following packages: DT, shiny, shinyWidgets, shinythemes, shinyjs, bslib, bsicons, lubridate, leaflet, highcharter…. Check out the shiny app"
  },
  {
    "objectID": "projects/moonr.html",
    "href": "projects/moonr.html",
    "title": "Packaging MOON in R",
    "section": "",
    "text": "The MOON (Modeling Obesity in Norway) study is a comprehensive decision-analytic model designed to simulate the long-term health and economic impacts of obesity in the Norwegian population. Built using a Markov framework, the model tracks individuals from age 2 to 100 across health states—normal weight, overweight, obese grade 1, obese grade 2, and death—based on real-world longitudinal data from national health surveys and registries. It estimates prevalence, healthcare costs, and years of life lost due to obesity, providing a robust foundation for evaluating the cost-effectiveness of preventive and treatment interventions.\n\n\nWe are currently undertaking a modernization of the MOON model by transitioning it from its original Excel-based implementation to a more scalable and transparent R-based framework. This shift will involve re-coding the model logic, integrating statistical analyses directly within R, and improving the reproducibility and flexibility of simulations. By leveraging R’s capabilities in data handling, visualization, and probabilistic modeling, we aim to enhance the model’s usability for researchers, policymakers, and public health analysts.\nMigrating the MOON model to R will significantly improve its accessibility, adaptability, and analytical power. The new version will allow for easier updates with new data, more sophisticated scenario analyses, and integration with other public health datasets. It will also enable other researchers—both within and outside Norway—to replicate, calibrate, and extend the model to their own contexts. Ultimately, this upgrade will support more informed decision-making in obesity prevention and treatment, helping stakeholders evaluate long-term health and economic outcomes with greater precision.\n\n\n\n Back to top"
  },
  {
    "objectID": "projects/moonr.html#packaging-moon",
    "href": "projects/moonr.html#packaging-moon",
    "title": "MSA",
    "section": "",
    "text": "Both R and Excel are powerful data science tools and can be used to accomplish end-to-end analytics projects.\nGudrun and I, are trying to move the decision analytics model built on excel to R, to promote open-access science. The decision analytics model runs a Markov model to assess the cost effectiveness of intervention for obesity."
  },
  {
    "objectID": "resume.html#summary",
    "href": "resume.html#summary",
    "title": "Curriculum Vitae",
    "section": "Summary",
    "text": "Summary\n\nPhD in Genetic Epidemiology, dual master’s in Biostatistics, and bachelor’s in Statistics.\nSix years of experience in data analysis in various statistical methods and datasets related to health.\nKnowledge of methods related to epidemiology, biostatistics, clinical trials, and genetics with hands-on experience of its implementation in both R and Excel.\nTechnically minded with the ability to break down a complex solution to ensure understanding at multiple levels and heterogeneous audience.\nCollaborative and supportive team player, conscious of my role as a contributor to a good working environment.\nAbility to take on new challenges in a fast-paced environment and make decisions under pressure.\nFluent in English and beginner in Norwegian"
  },
  {
    "objectID": "resume.html#professional-experience",
    "href": "resume.html#professional-experience",
    "title": "Curriculum Vitae",
    "section": "Professional Experience",
    "text": "Professional Experience\n\nAssociate, Parexel – Remote, India\nJuly 2021 – February 2022\n\nSupported the end-to-end development of a dynamic and interactive R Shiny application for conducting economic evaluations, visualizing results, and generating downloadable reports.\nPerformed a variety of statistical analyses to support evidence synthesis, utility estimation, and uncertainty assessments.\nDesigned and implemented dynamic dashboards to visualize key clinical endpoints, including survival outcomes, therapy lines, and demographic summaries, customized for regional stakeholder reporting.\nContributed to the development of statistical analysis plans by scripting analyses, conducting feasibility assessments, and validating datasets for accuracy and completeness.\nDelivered clear and actionable insights to internal and external stakeholders, incorporating feedback to improve deliverables."
  },
  {
    "objectID": "resume.html#research-experience",
    "href": "resume.html#research-experience",
    "title": "Curriculum Vitae",
    "section": "Research Experience",
    "text": "Research Experience\n\nPhD, NTNU – Trondheim, Norway\nApril 2022 – Current\n\nConducted genome-wide association studies to identify novel genetic associations with celiac disease, analyzing 24 million variants using established statistical pipelines.\nDeveloped a polygenic risk score (PRS) model that helped reclassify 7–11% of the population into different risk groups, enabling personalized approaches to medicine.\nIntegrated real-world data from patient registries, health surveys, and clinical databases to build predictive models by combining genetic and non-genetic information.\nPresented research findings at national and international conferences and published corresponding manuscripts in peer-reviewed journals.\nSecured more than 150,000 NOK in competitive external funding for a research stay abroad, supporting collaboration between university research groups.\n\n\n\nResearcher, NTNU – Trondheim, Norway\nApril 2025 – Current\n\nDeveloped a proof-of-concept R Shiny app to calculate absolute and proportional shortfall for Nordic countries, using dynamic inputs and comparative options for discount rates and EQ-5D value sets.\nPerformed regression analysis to infer the relationship between mental health patients’ referral rejections and patient/GP characteristics using real-world data.\nBuilt an R package to migrate existing decision analytics models for obesity from Excel to R, creating a lightweight, updatable tool that promotes open-access science."
  },
  {
    "objectID": "resume.html#project",
    "href": "resume.html#project",
    "title": "Curriculum Vitae",
    "section": "Project",
    "text": "Project\n\nNordic Shortfall Calculator\n\nDeveloped a proof-of-concept model that calculates the absolute and proportional shortfall of 4 Nordic countries, i.e., Denmark, Finland, Norway, and Sweden\nProvides up-to-date estimates of quality-adjusted life expectency of respective Nordic population based on the EQ-5D or 3D values.\nFeatures comparative options between different value sets for each country, discount rates, updates automatically with dynamic input, provides HRQoL, Cummulative Survival and Cummulative QALYs."
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Curriculum Vitae",
    "section": "Skills",
    "text": "Skills\n\nSoftware: R, SAS, Python, Git, Microsoft Excel, PowerPoint, Word\nTransferable: Communicator, Detail-oriented, Problem solver, Relationship builder, Results-driven, Strategic\nHobbies: Hiking, Skiing, Swimming, Anime"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Curriculum Vitae",
    "section": "Education",
    "text": "Education\n\nMPhil in Biostatistics and Demography, International Institute for Population Sciences (June 2020 – May 2021)\n\nMSc in Biostatistics and Demography, International Institute for Population Sciences (July 2018 – June 2020)\n\nBSc in Statistics, University of Calcutta (April 2014 – May 2018)"
  },
  {
    "objectID": "resume.html#publications",
    "href": "resume.html#publications",
    "title": "Curriculum Vitae",
    "section": "Publications",
    "text": "Publications\n\nPopulation screening of adults identifies novel genetic variants associated with celiac disease. Alam MS, Thomas L, Brumpton B, Hveem K, Lundin KE, Withoff S, Jonkers IH, Sollid LM, Hjort R, Ness-Jensen E. Scientific Reports. 2025 Jun 5;15(1):19764\nUnderstanding the spatial predictors of malnutrition among 0–2 years children in India using path analysis. Singh M, Alam MS, Majumdar P, Tiwary B, Narzari H, Mahendradhata Y. Frontiers in Public Health. 2021 Jul 30;9:667502\nThyroid Function Test in COVID-19 Patients: A Cross-Sectional Study in a Tertiary Care Hospital. Sen K, Sinha A, Sen S, Chakraborty S, Alam MS. Indian J Endocrinol Metab. 2020;24(6):532-536\nThe effects of social media consumption among the internet users during COVID-19 lockdown in India: Results from an online survey. Mustafa A, Alam MS, Shekhar C. Online Journal of Health and Allied Sciences 19 (4)"
  },
  {
    "objectID": "resume.html#conferences",
    "href": "resume.html#conferences",
    "title": "Curriculum Vitae",
    "section": "Conferences",
    "text": "Conferences\n\nOral\n\n20th International Celiac Disease Symposium – Sheffield, UK, September 2024\nNordic Conference on Future Health – Trondheim, Norway, September 2024\n16th International Conference on Urban Health – Xiamen, China, November 2019\n5th International Integrative Research Conference on Governance and Modernization in Changing Environment - Comilla, Bangladesh, December 2018\n\n\n\nPoster\n\n20th International Celiac Disease Symposium – Sheffield, UK, September 2024\nNordic Conference on Future Health – Trondheim, Norway, September 2024"
  },
  {
    "objectID": "resume.html#references",
    "href": "resume.html#references",
    "title": "Curriculum Vitae",
    "section": "References",
    "text": "References\nReferences can be provided on request."
  },
  {
    "objectID": "talks/iircgmce2019oral.html",
    "href": "talks/iircgmce2019oral.html",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "",
    "text": "5th International Integrative Research Conference on Governance and Modernization in Changing Environment (INSEARCH 2018)"
  },
  {
    "objectID": "talks/iircgmce2019oral.html#th-international-integrative-research-conference-on",
    "href": "talks/iircgmce2019oral.html#th-international-integrative-research-conference-on",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "",
    "text": "Governance and Modernization in Changing Environment (INSEARCH 2018)\nVenue: BARD, Comilla, Bangladesh\nBackground: Previously, studies have manifested a high magnitude of maternal deathsattributed to obstetric haemorrhage in West Bengal. But in the recent decade, the State has marked a unique recognition for having the highest maternal mortality caused by eclampsia in the country as well as in the globe.\nObjectives: The study aimed to determine the incidence of maternal mortality attributed to eclampsia and identify the confounding factors associated with eclamptic maternal deaths.\nMethods: It was a mixed method study. First, a facility-based study was conducted for all maternal deaths occurred between November 2013 and October 2015 (N=317), in two tertiary level hospitals. Second, we conducted community-based maternal death reviews (verbal autopsies) of 20 deaths.\nResults: Eclampsia accounted in one-third of maternal deaths. Almost two-thirds of maternal deaths attributed to eclampsia were highly concentrated among women who resided in distant areas from the studied hospitals. The women belonged to &lt;24 years age group (65%) and primigravidas (40%) mothers were noted to have an increased risk of eclamptic deaths as compared to relatively older groups and multigravida women. The maternal deaths related to eclampsia was comparatively higher among cesarean section experienced mother compared to normally delivered. Verbal autopsies indicated that majority of pregnant women had the irregular AN Cheek-ups history, particularly during the second trimester of pregnancy. Most of the eclamptic deceased women had been taken to at least three health facilities. Gravidity, the number of ANC and mode of delivery were the significant confounder of eclamptic deaths in the study.\nConclusion: The study confirmed an excess risk of hypertensive disorders which is much higher compared to previous studies in the globe. Establishment of separate Eclampsia Units at lower levels health facilities equipping with modern ICU facilities is more plausible and expedient pathway to alleviate the burden of eclampsia-related maternal deaths.\nKeywords: Eclampsia, Maternal Mortality, West Bengal"
  },
  {
    "objectID": "talks/iircgmce2019oral.html#th-international-integrative-research-conference-on-governance-and-modernization-in-changing-environment-insearch-2018",
    "href": "talks/iircgmce2019oral.html#th-international-integrative-research-conference-on-governance-and-modernization-in-changing-environment-insearch-2018",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "",
    "text": "Venue: BARD, Comilla, Bangladesh"
  },
  {
    "objectID": "talks/iircgmce2019oral.html#abstract",
    "href": "talks/iircgmce2019oral.html#abstract",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "Abstract",
    "text": "Abstract\nBackground: Previously, studies have manifested a high magnitude of maternal deathsattributed to obstetric haemorrhage in West Bengal. But in the recent decade, the State has marked a unique recognition for having the highest maternal mortality caused by eclampsia in the country as well as in the globe.\nObjectives: The study aimed to determine the incidence of maternal mortality attributed to eclampsia and identify the confounding factors associated with eclamptic maternal deaths.\nMethods: It was a mixed method study. First, a facility-based study was conducted for all maternal deaths occurred between November 2013 and October 2015 (N=317), in two tertiary level hospitals. Second, we conducted community-based maternal death reviews (verbal autopsies) of 20 deaths.\nResults: Eclampsia accounted in one-third of maternal deaths. Almost two-thirds of maternal deaths attributed to eclampsia were highly concentrated among women who resided in distant areas from the studied hospitals. The women belonged to &lt;24 years age group (65%) and primigravidas (40%) mothers were noted to have an increased risk of eclamptic deaths as compared to relatively older groups and multigravida women. The maternal deaths related to eclampsia was comparatively higher among cesarean section experienced mother compared to normally delivered. Verbal autopsies indicated that majority of pregnant women had the irregular AN Cheek-ups history, particularly during the second trimester of pregnancy. Most of the eclamptic deceased women had been taken to at least three health facilities. Gravidity, the number of ANC and mode of delivery were the significant confounder of eclamptic deaths in the study.\nConclusion: The study confirmed an excess risk of hypertensive disorders which is much higher compared to previous studies in the globe. Establishment of separate Eclampsia Units at lower levels health facilities equipping with modern ICU facilities is more plausible and expedient pathway to alleviate the burden of eclampsia-related maternal deaths.\nKeywords: Eclampsia, Maternal Mortality, West Bengal"
  },
  {
    "objectID": "talks/iircgmce2019oral.html#conference-name",
    "href": "talks/iircgmce2019oral.html#conference-name",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "",
    "text": "5th International Integrative Research Conference on Governance and Modernization in Changing Environment (INSEARCH 2018)"
  },
  {
    "objectID": "talks/iircgmce2019oral.html#conference-venue",
    "href": "talks/iircgmce2019oral.html#conference-venue",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nBARD, Comilla, Bangladesh"
  },
  {
    "objectID": "talks/insearch2018oral.html#conference-venue",
    "href": "talks/insearch2018oral.html#conference-venue",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nBARD, Comilla, Bangladesh"
  },
  {
    "objectID": "talks/insearch2018oral.html#abstract",
    "href": "talks/insearch2018oral.html#abstract",
    "title": "Shifting Paradigm in the Cause of Maternal Mortality in West Bengal: A Facility and Community-Based Mixed Method Approach Study",
    "section": "Abstract",
    "text": "Abstract\nBackground: Previously, studies have manifested a high magnitude of maternal deathsattributed to obstetric haemorrhage in West Bengal. But in the recent decade, the State has marked a unique recognition for having the highest maternal mortality caused by eclampsia in the country as well as in the globe.\nObjectives: The study aimed to determine the incidence of maternal mortality attributed to eclampsia and identify the confounding factors associated with eclamptic maternal deaths.\nMethods: It was a mixed method study. First, a facility-based study was conducted for all maternal deaths occurred between November 2013 and October 2015 (N=317), in two tertiary level hospitals. Second, we conducted community-based maternal death reviews (verbal autopsies) of 20 deaths.\nResults: Eclampsia accounted in one-third of maternal deaths. Almost two-thirds of maternal deaths attributed to eclampsia were highly concentrated among women who resided in distant areas from the studied hospitals. The women belonged to &lt;24 years age group (65%) and primigravidas (40%) mothers were noted to have an increased risk of eclamptic deaths as compared to relatively older groups and multigravida women. The maternal deaths related to eclampsia was comparatively higher among cesarean section experienced mother compared to normally delivered. Verbal autopsies indicated that majority of pregnant women had the irregular AN Cheek-ups history, particularly during the second trimester of pregnancy. Most of the eclamptic deceased women had been taken to at least three health facilities. Gravidity, the number of ANC and mode of delivery were the significant confounder of eclamptic deaths in the study.\nConclusion: The study confirmed an excess risk of hypertensive disorders which is much higher compared to previous studies in the globe. Establishment of separate Eclampsia Units at lower levels health facilities equipping with modern ICU facilities is more plausible and expedient pathway to alleviate the burden of eclampsia-related maternal deaths.\nKeywords: Eclampsia, Maternal Mortality, West Bengal"
  },
  {
    "objectID": "talks/icuh2020oral.html#conference-venue",
    "href": "talks/icuh2020oral.html#conference-venue",
    "title": "Prevalence of communicable diseases and its epidemiological correlates in major urban populated states of India: Evidence from a nationally representative sample",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nXiamen, China"
  },
  {
    "objectID": "talks/icuh2020oral.html#abstract",
    "href": "talks/icuh2020oral.html#abstract",
    "title": "Prevalence of communicable diseases and its epidemiological correlates in major urban populated states of India: Evidence from a nationally representative sample",
    "section": "Abstract",
    "text": "Abstract\nIntroduction: Poor water quality has been considered to be a confounder factor of the communicable disease. The study analyzed the current burden of communicable diseases and explored the factors associated with it in the major urban populated states of India.\nMethods: This study used the India’s fourth round District Level Household and Facility Survey (2012–13) data. The study particularly focused on the major urban states of India which are having more than 30,000 urban populations. We used bivariate analysis and the adjusted odds ratio (AOR) with 95% confidence intervals (CI) to identify the independent predictors.\nResults: Public tap was found to be a prime source of drinking water in the urban India. Tamil Nadu (near about half) had the largest urban population using unimproved water sources. The prevalence of communicable disease was high (11 percent) among the population using unimproved drinking compared to their counterparts (5 percent). About 15 percent of the urban population in West Bengal had higher communicable diseases than other urban populated states of India. The factors found significantly associated with communicable diseases were age (AOR=1.40), years of schooling (AOR=0.95), source of drinking water (AOR=0.77) and having toilet facility (AOR=1.32).\nConclusion: Improved drinking water coverage in India is still considerably low as compared to the national target set in SDG. The present study suggests that improved drinking water sources and clean and hygienic toilet facility are significant determinants of communicable diseases. The measure should be devised to enhance health information and accessibility of safe drinking water to prevent communicable diseases in line with achieving the Sustainable Development Goal 3.\nKeywords: Communicable diseases, risk factors, urban area, India."
  },
  {
    "objectID": "talks/icuh2019oral.html#conference-venue",
    "href": "talks/icuh2019oral.html#conference-venue",
    "title": "Prevalence of communicable diseases and its epidemiological correlates in major urban populated states of India: Evidence from a nationally representative sample",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nXiamen, China"
  },
  {
    "objectID": "talks/icuh2019oral.html#abstract",
    "href": "talks/icuh2019oral.html#abstract",
    "title": "Prevalence of communicable diseases and its epidemiological correlates in major urban populated states of India: Evidence from a nationally representative sample",
    "section": "Abstract",
    "text": "Abstract\nIntroduction: Poor water quality has been considered to be a confounder factor of the communicable disease. The study analyzed the current burden of communicable diseases and explored the factors associated with it in the major urban populated states of India.\nMethods: This study used the India’s fourth round District Level Household and Facility Survey (2012–13) data. The study particularly focused on the major urban states of India which are having more than 30,000 urban populations. We used bivariate analysis and the adjusted odds ratio (AOR) with 95% confidence intervals (CI) to identify the independent predictors.\nResults: Public tap was found to be a prime source of drinking water in the urban India. Tamil Nadu (near about half) had the largest urban population using unimproved water sources. The prevalence of communicable disease was high (11 percent) among the population using unimproved drinking compared to their counterparts (5 percent). About 15 percent of the urban population in West Bengal had higher communicable diseases than other urban populated states of India. The factors found significantly associated with communicable diseases were age (AOR=1.40), years of schooling (AOR=0.95), source of drinking water (AOR=0.77) and having toilet facility (AOR=1.32).\nConclusion: Improved drinking water coverage in India is still considerably low as compared to the national target set in SDG. The present study suggests that improved drinking water sources and clean and hygienic toilet facility are significant determinants of communicable diseases. The measure should be devised to enhance health information and accessibility of safe drinking water to prevent communicable diseases in line with achieving the Sustainable Development Goal 3.\nKeywords: Communicable diseases, risk factors, urban area, India."
  },
  {
    "objectID": "talks/icds24poster.html#conference-venue",
    "href": "talks/icds24poster.html#conference-venue",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nSheffield, UK"
  },
  {
    "objectID": "talks/icds24poster.html#abstract",
    "href": "talks/icds24poster.html#abstract",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "Abstract",
    "text": "Abstract\nIntroduction: Despite diagnostic advances in celiac disease (CeD), many patients remain undiagnosed. CeD has well established genetic risk factors in the leukocyte antigen (HLA) loci. The goal of the study was to provide susceptibility estimates for CeD subgroups using polygenic risk score (PRS) beyond the HLA loci.\nMethods: In the population-based HUNT study in Norway, 52,588 adults underwent CeD screening via serology with diagnosis confirmed by histology (revealing 465 incident [Marsh 3] and 230 potential [Marsh 1/2] cases). Additionally, 377 known CeD cases were identified from medical registries. We reproduced a previously published PRS of CeD (228 SNPs) using the PRS-cs tool. All analysis were adjusted for age, sex, genotyping batch and 20 principal components.\nResults: The PRS could effectively distinguish between incident and prevalent cases from controls, with area under receiver operating characteristic curves at 83.8% and 83.5%, respectively, superior to potential cases (68.8%). For every standard deviation increase in the PRS, the odds increased 3.4-times (95% confidence interval [CI] 3.1-3.8) for confirmed (incident and prevalent) and 1.8-times (CI 1.6-2.1) for potential cases. Individuals in the top vs remaining decile of the PRS had 8.4-times (CI 7.3-9.7) higher odds of CeD. The proportion of variation explained by the PRS was 20.9% (CI 17.2%-25.6%) from HLA and 1% (CI 0.2%-2.3%) from non-HLA.\nConclusions: Incorporating non-HLA variants slightly enhanced identification of CeD beyond HLA variants alone, highlighting the potential of genetic risk stratification, integrating both HLA and non-HLA variants to pinpoint high-risk individuals.\nKeywords: polygenic, symptoms, PRS, onset, genetic, autoimmune"
  },
  {
    "objectID": "talks/ncfh24poster.html#conference-venue",
    "href": "talks/ncfh24poster.html#conference-venue",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nTrondheim, Norway"
  },
  {
    "objectID": "talks/ncfh24poster.html#abstract",
    "href": "talks/ncfh24poster.html#abstract",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "Abstract",
    "text": "Abstract\nIntroduction: Previous studies have discovered genetic loci associated with celiac disease (CeD), within both the human leukocyte antigen (HLA) and the non-HLA region. Yet, half of the genetic variation remains unexplained. This study aims to investigate novel associations with CeD taking advantage of a screened population and mitigating selection-bias from prior case-control studies.\nMethods: Utilizing data from the HUNT study in Norway, we screened 52,358 adults (&gt;20 years) for CeD using serology, identifying 465 incident biopsy-confirmed cases. Additionally, 377 prevalent cases were identified through hospital journal searches. Genotyping of 373,185 SNPs was performed using four Illumina HumanCoreExome arrays. Imputation using the Haplotype Reference Consortium panel, resulted in approximately 24.9 million variants, post quality control. A genome-wide association study was performed using SAIGE, and functional mapping and pathway enrichment analysis was conducted using FUMA.\nResults: Five novel SNPs out of 6 independent associations reached genome wide significance (\\(P≤5E-08\\)) having minor allele count greater than 10. EML6, BCL11A, ABCA12, LRFN2, MED13, IRX1 are the loci respectively. Around 50 of over 80 loci identified in previous GWASs were replicated in our data, with REL, ASHA2, IL18RAP, IL18R1, IL18RL1, IL18RL2, LPP, PFKFB3, PRKCQ, CIITA, SOCS1, and CLEC16A reaching suggestive significance levels (\\(5E-08≤P≤5E-06\\)).\nConclusion: The strongest evidence for an association was observed at IRX1, warranting further studies to validate this finding. Notably, the IRX1 loci has also been associated with other autoimmune diseases such as rheumatoid arthritis.\nKeywords: celiac, gluten, non-HLA, autoimmune, GWAS."
  },
  {
    "objectID": "talks/ncfh24oral.html#conference-venue",
    "href": "talks/ncfh24oral.html#conference-venue",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nTrondheim, Norway"
  },
  {
    "objectID": "talks/ncfh24oral.html#abstract",
    "href": "talks/ncfh24oral.html#abstract",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "Abstract",
    "text": "Abstract\nIntroduction: Despite diagnostic advances in celiac disease (CeD), many patients remain undiagnosed. CeD has well established genetic risk factors in the leukocyte antigen (HLA) loci. The goal of the study was to provide susceptibility estimates for CeD subgroups using polygenic risk score (PRS) beyond the HLA loci.\nMethods: In the population-based HUNT study in Norway, 52,588 adults underwent CeD screening via serology with diagnosis confirmed by histology (revealing 465 incident [Marsh 3] and 230 potential [Marsh 1/2] cases). Additionally, 377 known CeD cases were identified from medical registries. We reproduced a previously published PRS of CeD (228 SNPs) using the PRS-cs tool. All analysis were adjusted for age, sex, genotyping batch and 20 principal components.\nResults: The PRS could effectively distinguish between incident and prevalent cases from controls, with area under receiver operating characteristic curves at 83.8% and 83.5%, respectively, superior to potential cases (68.8%). For every standard deviation increase in the PRS, the odds increased 3.4-times (95% confidence interval [CI] 3.1-3.8) for confirmed (incident and prevalent) and 1.8-times (CI 1.6-2.1) for potential cases. Individuals in the top vs remaining decile of the PRS had 8.4-times (CI 7.3-9.7) higher odds of CeD. The proportion of variation explained by the PRS was 20.9% (CI 17.2%-25.6%) from HLA and 1% (CI 0.2%-2.3%) from non-HLA.\nConclusions: Incorporating non-HLA variants slightly enhanced identification of CeD beyond HLA variants alone, highlighting the potential of genetic risk stratification, integrating both HLA and non-HLA variants to pinpoint high-risk individuals.\nKeywords: polygenic, symptoms, PRS, onset, genetic, autoimmune"
  },
  {
    "objectID": "talks/icds24oral.html#conference-venue",
    "href": "talks/icds24oral.html#conference-venue",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "Conference Venue:",
    "text": "Conference Venue:\nSheffield, UK"
  },
  {
    "objectID": "talks/icds24oral.html#abstract",
    "href": "talks/icds24oral.html#abstract",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "Abstract",
    "text": "Abstract\nIntroduction: Previous studies have discovered genetic loci associated with celiac disease (CeD), within both the human leukocyte antigen (HLA) and the non-HLA region. Yet, half of the genetic variation remains unexplained. This study aims to investigate novel associations with CeD taking advantage of a screened population and mitigating selection-bias from prior case-control studies.\nMethods: Utilizing data from the HUNT study in Norway, we screened 52,358 adults (&gt;20 years) for CeD using serology, identifying 465 incident biopsy-confirmed cases. Additionally, 377 prevalent cases were identified through hospital journal searches. Genotyping of 373,185 SNPs was performed using four Illumina HumanCoreExome arrays. Imputation using the Haplotype Reference Consortium panel, resulted in approximately 24.9 million variants, post quality control. A genome-wide association study was performed using SAIGE, and functional mapping and pathway enrichment analysis was conducted using FUMA.\nResults: Five novel SNPs out of 6 independent associations reached genome wide significance (\\(P≤5E-08\\)) having minor allele count greater than 10. EML6, BCL11A, ABCA12, LRFN2, MED13, IRX1 are the loci respectively. Around 50 of over 80 loci identified in previous GWASs were replicated in our data, with REL, ASHA2, IL18RAP, IL18R1, IL18RL1, IL18RL2, LPP, PFKFB3, PRKCQ, CIITA, SOCS1, and CLEC16A reaching suggestive significance levels (\\(5E-08≤P≤5E-06\\)).\nConclusion: The strongest evidence for an association was observed at IRX1, warranting further studies to validate this finding. Notably, the IRX1 loci has also been associated with other autoimmune diseases such as rheumatoid arthritis.\nKeywords: celiac, gluten, non-HLA, autoimmune, GWAS."
  },
  {
    "objectID": "talks/icds24oral.html",
    "href": "talks/icds24oral.html",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "",
    "text": "View presentation"
  },
  {
    "objectID": "talks/icds24oral.html#conference-name",
    "href": "talks/icds24oral.html#conference-name",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "Conference name",
    "text": "Conference name\n\nInternational Coeliac Disease Symposium (ICDS)"
  },
  {
    "objectID": "talks/ncfh24poster.html",
    "href": "talks/ncfh24poster.html",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "",
    "text": "View poster"
  },
  {
    "objectID": "talks/ncfh24poster.html#conference-name",
    "href": "talks/ncfh24poster.html#conference-name",
    "title": "The IRX1 locus is associated with celiac disease: results from a screened population-based cohort, the HUNT study.",
    "section": "Conference name",
    "text": "Conference name\n\nThe Nordic Conference on Future Health"
  },
  {
    "objectID": "talks/ncfh24oral.html",
    "href": "talks/ncfh24oral.html",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "",
    "text": "View presentation"
  },
  {
    "objectID": "talks/ncfh24oral.html#conference-name",
    "href": "talks/ncfh24oral.html#conference-name",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "Conference name",
    "text": "Conference name\n\nThe Nordic Conference on Future Health"
  },
  {
    "objectID": "talks/icds24poster.html",
    "href": "talks/icds24poster.html",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "",
    "text": "View poster"
  },
  {
    "objectID": "talks/icds24poster.html#conference-name",
    "href": "talks/icds24poster.html#conference-name",
    "title": "Assessing the susceptibility of celiac disease by polygenic risk scores: analysis of a population-based cohort, the HUNT study.",
    "section": "Conference name",
    "text": "Conference name\n\nInternational Coeliac Disease Symposium (ICDS)"
  },
  {
    "objectID": "blogs/mice.html",
    "href": "blogs/mice.html",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "",
    "text": "Missing data is a common issue in real-world datasets. Ignoring or improperly handling missing values can lead to biased results and incorrect conclusions. One powerful technique to address this is Multiple Imputation, and in R, the mice package makes it accessible and effective.\nThis post will walk you through:\n\nWhat Multiple Imputation is\nWhy it’s useful\nHow to use the mice package in R\nDifferent imputation methods and their assumptions\nLimitations of Multiple Imputation"
  },
  {
    "objectID": "blogs/mice.html#introduction",
    "href": "blogs/mice.html#introduction",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "",
    "text": "Missing data is a common issue in real-world datasets. Ignoring or improperly handling missing values can lead to biased results and incorrect conclusions. One powerful technique to address this is Multiple Imputation, and in R, the mice package makes it accessible and effective.\nThis post will walk you through:\n\nWhat Multiple Imputation is\nWhy it’s useful\nHow to use the mice package in R\nDifferent imputation methods and their assumptions\nLimitations of Multiple Imputation"
  },
  {
    "objectID": "blogs/mice.html#what-is-multiple-imputation",
    "href": "blogs/mice.html#what-is-multiple-imputation",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "What is Multiple Imputation?",
    "text": "What is Multiple Imputation?\nMultiple Imputation is a statistical technique where missing values are filled in multiple times to create several complete datasets. Each dataset is analyzed separately, and the results are combined to account for the uncertainty due to missing data."
  },
  {
    "objectID": "blogs/mice.html#why-use-multiple-imputation",
    "href": "blogs/mice.html#why-use-multiple-imputation",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "Why Use Multiple Imputation?",
    "text": "Why Use Multiple Imputation?\n\nPreserves sample size\nReduces bias compared to single imputation\nAccounts for uncertainty in missing data\nWorks well with many types of data"
  },
  {
    "objectID": "blogs/mice.html#getting-started-with-mice-in-r",
    "href": "blogs/mice.html#getting-started-with-mice-in-r",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "Getting Started with mice in R",
    "text": "Getting Started with mice in R\n0. Install and Load the Package\n\nCode# install.packages(\"mice\")\n# install.packages(\"gt\")\nlibrary(mice)\nlibrary(gt)\n# example dataset: airquality\ndata(\"airquality\")\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nCodesummary(airquality)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\n\nThis dataset contains missing values in Ozone and Solar.R.\n1. Check missing data pattern\n\nCodemd.pattern(airquality)\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\n\n2. Perform imputations\n\nCodeimp &lt;- mice(airquality, m = 5, method = 'pmm', seed = 123)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  1   2  Ozone  Solar.R\n  1   3  Ozone  Solar.R\n  1   4  Ozone  Solar.R\n  1   5  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  2   2  Ozone  Solar.R\n  2   3  Ozone  Solar.R\n  2   4  Ozone  Solar.R\n  2   5  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n  3   2  Ozone  Solar.R\n  3   3  Ozone  Solar.R\n  3   4  Ozone  Solar.R\n  3   5  Ozone  Solar.R\n  4   1  Ozone  Solar.R\n  4   2  Ozone  Solar.R\n  4   3  Ozone  Solar.R\n  4   4  Ozone  Solar.R\n  4   5  Ozone  Solar.R\n  5   1  Ozone  Solar.R\n  5   2  Ozone  Solar.R\n  5   3  Ozone  Solar.R\n  5   4  Ozone  Solar.R\n  5   5  Ozone  Solar.R\n\n\nm = 5: Number of imputed datasets\nmethod = 'pmm': Predictive Mean Matching (default for numeric data)\n\nImputation methods within mice()\n\n\nMethod\nDescription\nAssumptions\n\n\n\npmm\nPredictive Mean\nMatching Data is approximately normal\n\n\nnorm\nBayesian linear regression\nContinuous data\n\n\nlogreg\nLogistic regression\nBinary data\n\n\npolyreg\nPolytomous regression\nCategorical data\n\n\ncart\nClassification and regression trees\nNon-parametric\n\n\n3. Check imputed values\n\nCodeimp$imp$Ozone\n\n      1   2   3   4   5\n5    18   1  37  18  32\n10   12  30  30  20  20\n25   18   8   8  28   6\n26   13  18  18  20  13\n27   20  11  21  32  20\n32   16  23  44  49  59\n33   12   7  18  39  59\n34   19  37  28  18  32\n35   52  39  20  59  96\n36    7  85  71  96  80\n37   21  18  24  29  31\n39   64  76  61  76  76\n42   64  76 115  97 122\n43   61  97  80  76  97\n45   30  29  31  35  28\n46   13  63  52  37  63\n52   23  71  23  35  23\n53   85  64  64  40 108\n54   37  59  47  20  49\n55   23  37  71  35  89\n56   29  29  28  44  46\n57   47  23  35  40  63\n58   44  23  22  23  23\n59   46  45  36  20  28\n60   11  34   9  41   7\n61   78  64  78  73 108\n65   18  20  36  16  32\n72   52  35  16  44  35\n75   40  96  46  61  37\n83    7  59  35  71  23\n84   29  63  20  89  40\n102 110  77 108  66  82\n103  28  40  13  52  35\n107  16  16  14  21  23\n115  22  23  23  16  24\n119 122 122  85 135  82\n150  27  44  14  12  13\n\n\n4. Complete the data\n\nCodecompleted_data &lt;- complete(imp, 1)  # Get the first imputed dataset\nhead(completed_data)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    18     150 14.3   56     5   5\n6    28      48 14.9   66     5   6\n\n\n4. Analyze and pool the results\n\nCodefit &lt;- with(data = imp, exp = lm(Ozone ~ Solar.R + Wind + Temp))\npooled &lt;- pool(fit)\n\npooled_summary &lt;- summary(pooled)\n\n# Remove the 'statistic' and 'df' column\npooled_summary &lt;- pooled_summary[, !(names(pooled_summary) %in% c(\"statistic\", \"df\"))]\n\n# Create a gt table\ngt_table &lt;- pooled_summary |&gt;\n  gt() |&gt;\n  tab_header(\n    title = \"Pooled Regression Results\",\n    subtitle = \"Using Multiple Imputation with mice\"\n  ) |&gt;\n  fmt_number(\n    columns = vars(estimate, std.error, p.value),\n    decimals = 3\n  ) |&gt;\n  cols_label(\n    term = \"Term\",\n    estimate = \"Estimate\",\n    std.error = \"Std. Error\",\n    p.value = \"p-Value\"\n  )\n\ngt_table\n\n\n\n\n\n\n\nPooled Regression Results\n\n\nUsing Multiple Imputation with mice\n\n\nTerm\nEstimate\nStd. Error\np-Value\n\n\n\n\n(Intercept)\n−62.907\n22.861\n0.009\n\n\nSolar.R\n0.053\n0.026\n0.059\n\n\nWind\n−3.075\n0.655\n0.000\n\n\nTemp\n1.618\n0.241\n0.000"
  },
  {
    "objectID": "blogs/mice.html#step-by-step-imputation",
    "href": "blogs/mice.html#step-by-step-imputation",
    "title": "Beginner’s Guide to Multiple Imputation Using the mice Package in R",
    "section": "Step-by-step imputation",
    "text": "Step-by-step imputation\n\n1. Check missing data pattern\nmd.pattern(airquality)\n\n\n2. Perform imputations\nimp &lt;- mice(airquality, m = 5, method = 'pmm', seed = 123)\nm = 5: Number of imputed datasets\nmethod = 'pmm': Predictive Mean Matching (default for numeric data)\nImputation methods in mice()\n\n\n3. Check imputed values\nimp$imp$Ozone\n\n\n4. Complete the data\ncompleted_data &lt;- complete(imp, 1)  # Get the first imputed dataset\nhead(completed_data)\n\n\n4. Analyze and pool the results\nfit &lt;- with(data = imp, exp = lm(Ozone ~ Solar.R + Wind + Temp))\npooled &lt;- pool(fit)\nsummary(pooled)\n\n\nLimitations of Multiple Imputation\n\nComputationally intensive\nRequires careful model specification\nAssumes data is Missing at Random (MAR) — not always true\nCan be complex for high-dimensional data\n\n\n\nConclusion\nMultiple Imputation is a robust way to handle missing data, and the mice package in R makes it approachable even for beginners. By understanding the methodology and using appropriate imputation methods, you can improve the quality of your analyses significantly.\n\n\n🧵 TL;DR: Multiple Imputation with mice in R\n\nProblem: Missing data can bias results and reduce statistical power.\nSolution: Multiple Imputation fills in missing values multiple times to reflect uncertainty.\nTool: The mice package in R makes this easy and flexible.\nSteps:\n\nInspect missing data (md.pattern)\nImpute with mice() using methods like pmm, norm, cart, etc.\nAnalyze each imputed dataset\nPool results for final inference\nUse Cases: Surveys, clinical trials, social science data, etc.\n\nAssumptions: Data is Missing at Random (MAR)\nLimitations: Computational cost, model sensitivity, assumption of MAR"
  },
  {
    "objectID": "blogs/mice.html#limitations-of-multiple-imputation",
    "href": "blogs/mice.html#limitations-of-multiple-imputation",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "Limitations of Multiple Imputation",
    "text": "Limitations of Multiple Imputation\n\nComputationally intensive\nRequires careful model specification\nAssumes data is Missing at Random (MAR) — not always true\nCan be complex for high-dimensional data"
  },
  {
    "objectID": "blogs/mice.html#conclusion",
    "href": "blogs/mice.html#conclusion",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "Conclusion",
    "text": "Conclusion\nMultiple Imputation is a robust way to handle missing data, and the mice package in R makes it approachable even for beginners. By understanding the methodology and using appropriate imputation methods, you can improve the quality of your analyses significantly."
  },
  {
    "objectID": "blogs/mice.html#tldr-multiple-imputation-with-mice-in-r",
    "href": "blogs/mice.html#tldr-multiple-imputation-with-mice-in-r",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "🧵 TL;DR: Multiple Imputation with mice in R",
    "text": "🧵 TL;DR: Multiple Imputation with mice in R\n\nProblem: Missing data can bias results and reduce statistical power.\nSolution: Multiple Imputation fills in missing values multiple times to reflect uncertainty.\nTool: The mice package in R makes this easy and flexible.\nSteps:\n\nInspect missing data (md.pattern)\nImpute with mice() using methods like pmm, norm, cart, etc.\nAnalyze each imputed dataset\nPool results for final inference\nUse Cases: Surveys, clinical trials, social science data, etc.\n\n\nAssumptions: Data is Missing at Random (MAR)\nLimitations: Computational cost, model sensitivity, assumption of MAR"
  },
  {
    "objectID": "blogs/gitsetup.html#what-youll-learn",
    "href": "blogs/gitsetup.html#what-youll-learn",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "",
    "text": "Whether you’re a beginner developer or switching to a new machine, setting up Git with SSH authentication can seem daunting. This comprehensive guide will walk you through every step to get you from a fresh installation to pushing your first commit securely.\nBy the end of this guide, you’ll be able to: - Configure Git with your identity - Generate and set up SSH keys for secure authentication - Create and manage local Git repositories - Connect your local work to remote repositories (GitHub, GitLab, etc.) - Understand the basic Git workflow for daily development"
  },
  {
    "objectID": "blogs/gitsetup.html#prerequisites",
    "href": "blogs/gitsetup.html#prerequisites",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nA computer with terminal/command line access\nAn account on a Git hosting service (GitHub, GitLab, Bitbucket, etc.)\nBasic familiarity with command line operations"
  },
  {
    "objectID": "blogs/gitsetup.html#step-1-install-and-configure-git",
    "href": "blogs/gitsetup.html#step-1-install-and-configure-git",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Step 1: Install and Configure Git",
    "text": "Step 1: Install and Configure Git\n\nInstall Git\nIf you haven’t already, download and install Git from the official website. Most Linux distributions and macOS come with Git pre-installed, but it’s worth updating to the latest version.\nCheck if Git is installed:\ngit --version\n\n\nConfigure Your Git Identity\nThis is crucial - Git needs to know who you are for commit attribution:\ngit config --global user.name \"Your Full Name\"\ngit config --global user.email \"your.email@example.com\"\nPro tip: Use the same email address associated with your GitHub/GitLab account for seamless integration.\nVerify your configuration:\ngit config --global --list\n\n\nOptional: Set Your Default Editor\nConfigure your preferred text editor for commit messages:\n# For VS Code\ngit config --global core.editor \"code --wait\"\n\n# For nano (beginner-friendly)\ngit config --global core.editor \"nano\"\n\n# For vim\ngit config --global core.editor \"vim\""
  },
  {
    "objectID": "blogs/gitsetup.html#step-2-set-up-ssh-keys-for-secure-authentication",
    "href": "blogs/gitsetup.html#step-2-set-up-ssh-keys-for-secure-authentication",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Step 2: Set Up SSH Keys for Secure Authentication",
    "text": "Step 2: Set Up SSH Keys for Secure Authentication\nSSH keys provide a secure way to authenticate with Git hosting services without entering your password every time.\n\nGenerate Your SSH Key\nCreate a new SSH key pair using the modern Ed25519 algorithm:\nssh-keygen -t ed25519 -C \"your.email@example.com\"\nWhen prompted: - File location: Press Enter to accept the default (~/.ssh/id_ed25519) - Passphrase: Optional but recommended for extra security\n\n\nStart the SSH Agent\nThe SSH agent manages your keys in memory:\neval \"$(ssh-agent -s)\"\nYou should see output like: Agent pid 12345\n\n\nAdd Your Key to the SSH Agent\nssh-add ~/.ssh/id_ed25519\nIf you set a passphrase, you’ll be prompted to enter it.\n\n\nCopy Your Public Key\nDisplay and copy your public key:\ncat ~/.ssh/id_ed25519.pub\nCopy the entire output (starts with ssh-ed25519 and ends with your email).\n\n\nAdd Key to Your Git Hosting Service\n\nFor GitHub:\n\nGo to Settings → SSH and GPG keys\nClick New SSH key\nGive it a descriptive title (e.g., “My Laptop”)\nPaste your public key\nClick Add SSH key\n\n\n\nFor GitLab:\n\nGo to Preferences → SSH Keys\nPaste your public key in the Key field\nAdd a title and expiration date (optional)\nClick Add key\n\n\n\n\nTest Your SSH Connection\nVerify your setup works:\n# For GitHub\nssh -T git@github.com\n\n# For GitLab\nssh -T git@gitlab.com\nYou should see a welcome message confirming successful authentication."
  },
  {
    "objectID": "blogs/gitsetup.html#step-3-create-your-first-local-repository",
    "href": "blogs/gitsetup.html#step-3-create-your-first-local-repository",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Step 3: Create Your First Local Repository",
    "text": "Step 3: Create Your First Local Repository\n\nNavigate to Your Project Directory\ncd /path/to/your/project\n# or create a new directory\nmkdir my-awesome-project\ncd my-awesome-project\n\n\nInitialize Git Repository\ngit init\nThis creates a hidden .git folder that tracks your project’s history.\n\n\nCreate Your First Files\nIf starting from scratch:\n# Create a README file\necho \"# My Awesome Project\" &gt; README.md\n\n# Create a simple .gitignore\ncat &gt; .gitignore &lt;&lt; EOF\n# OS generated files\n.DS_Store\nThumbs.db\n\n# IDE files\n.vscode/\n.idea/\n\n# Dependencies\nnode_modules/\nEOF\n\n\nStage and Commit Your Changes\n# Stage all files\ngit add .\n\n# Create your first commit\ngit commit -m \"Initial commit: Add README and .gitignore\"\nGood commit message practices: - Use present tense (“Add feature” not “Added feature”) - Keep the first line under 50 characters - Be descriptive but concise"
  },
  {
    "objectID": "blogs/gitsetup.html#step-4-connect-to-remote-repository",
    "href": "blogs/gitsetup.html#step-4-connect-to-remote-repository",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Step 4: Connect to Remote Repository",
    "text": "Step 4: Connect to Remote Repository\n\nCreate Remote Repository\n\nGo to your Git hosting service (GitHub/GitLab)\nCreate a new repository\nImportant: Don’t initialize with README, .gitignore, or license if you already have local commits\n\n\n\nLink Your Local Repository\ngit remote add origin git@github.com:yourusername/your-repo-name.git\n# Replace with your actual username and repository name\n\n\nCheck Your Default Branch\nModern Git uses main as the default branch, but older repos might use master:\ngit branch\nIf you’re on master but want to use main:\ngit branch -M main\n\n\nPush Your Code\nPush your local commits to the remote repository:\n# First push (sets up tracking)\ngit push -u origin main\n\n# Future pushes (after the -u flag is set)\ngit push\nThe -u flag (upstream) tells Git to remember this branch relationship, so future git push commands know where to go."
  },
  {
    "objectID": "blogs/gitsetup.html#daily-git-workflow",
    "href": "blogs/gitsetup.html#daily-git-workflow",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Daily Git Workflow",
    "text": "Daily Git Workflow\nOnce everything is set up, your typical workflow will be:\n# Make changes to your files\n# ...\n\n# Check what's changed\ngit status\n\n# Stage changes\ngit add .\n# or stage specific files\ngit add filename.txt\n\n# Commit with a descriptive message\ngit commit -m \"Add user authentication feature\"\n\n# Push to remote\ngit push\n\nUseful Git Commands\n# View commit history\ngit log --oneline\n\n# Check current status\ngit status\n\n# See what changes you've made\ngit diff\n\n# Undo changes to a file\ngit checkout -- filename.txt\n\n# Pull latest changes from remote\ngit pull"
  },
  {
    "objectID": "blogs/gitsetup.html#troubleshooting-common-issues",
    "href": "blogs/gitsetup.html#troubleshooting-common-issues",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\nPermission Denied (SSH)\n\nEnsure your SSH key is added to ssh-agent: ssh-add -l\nVerify the key is added to your Git hosting service\nTest SSH connection: ssh -T git@github.com\n\n\n\nRemote Already Exists\nIf you get “remote origin already exists”:\ngit remote remove origin\ngit remote add origin git@github.com:username/repo.git\n\n\nWrong Branch Name\nIf your local branch doesn’t match remote expectations:\n# Rename current branch\ngit branch -M main\ngit push -u origin main"
  },
  {
    "objectID": "blogs/gitsetup.html#security-best-practices",
    "href": "blogs/gitsetup.html#security-best-practices",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Security Best Practices",
    "text": "Security Best Practices\n\nUse SSH keys instead of passwords for authentication\nSet passphrases on your SSH keys for additional security\nRegularly rotate your SSH keys (annually recommended)\nNever commit sensitive data like passwords or API keys\nUse .gitignore to exclude sensitive files\nReview commits before pushing to avoid accidentally including secrets"
  },
  {
    "objectID": "blogs/gitsetup.html#next-steps",
    "href": "blogs/gitsetup.html#next-steps",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Next Steps",
    "text": "Next Steps\nNow that you have Git and SSH configured:\n\nLearn about branching and merging for collaborative development\nExplore pull requests/merge requests for code review workflows\nSet up automated backups of your repositories\nConsider using Git hooks for automated testing\nLearn about conventional commits for better commit messages"
  },
  {
    "objectID": "blogs/gitsetup.html#conclusion",
    "href": "blogs/gitsetup.html#conclusion",
    "title": "Complete Git and SSH Setup Guide: From Zero to Push",
    "section": "Conclusion",
    "text": "Conclusion\nCongratulations! You’ve successfully set up Git with SSH authentication and created your first repository. This foundation will serve you well throughout your development journey. Remember, Git is a powerful tool that becomes more valuable as you learn its advanced features, but these basics will handle 90% of your daily needs.\nHappy coding! 🚀"
  },
  {
    "objectID": "blogs/mice.html#tldr",
    "href": "blogs/mice.html#tldr",
    "title": "From Missing to Meaningful: Beginner’s Guide to Multiple Imputation",
    "section": "TL;DR",
    "text": "TL;DR\n\nProblem: Missing data can bias results and reduce statistical power.\nSolution: Multiple Imputation fills in missing values multiple times to reflect uncertainty.\nTool: The mice package in R makes this easy and flexible.\nSteps:\n\nInspect missing data (md.pattern)\nImpute with mice() using methods like pmm, norm, cart, etc.\nAnalyze each imputed dataset\nPool results for final inference\nUse Cases: Surveys, clinical trials, social science data, etc.\n\n\nAssumptions: Data is Missing at Random (MAR)\nLimitations: Computational cost, model sensitivity, assumption of MAR"
  },
  {
    "objectID": "blogs/gwas.html",
    "href": "blogs/gwas.html",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "",
    "text": "Genome-wide association studies (GWAS) are powerful tools for uncovering genetic variants linked to traits and diseases. After preliminary quality control (QC), additional checks and thoughtful modeling are essential—especially when dealing with related individuals and population structure.\nThis guide walks through:\n\nPost-QC checks\nPhenotype and covariate preparation\nGWAS using SAIGE\nAlternatives to SAIGE\nVisualization and interpretation"
  },
  {
    "objectID": "blogs/gwas.html#introduction",
    "href": "blogs/gwas.html#introduction",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "",
    "text": "Genome-wide association studies (GWAS) are powerful tools for uncovering genetic variants linked to traits and diseases. After preliminary quality control (QC), additional checks and thoughtful modeling are essential—especially when dealing with related individuals and population structure.\nThis guide walks through:\n\nPost-QC checks\nPhenotype and covariate preparation\nGWAS using SAIGE\nAlternatives to SAIGE\nVisualization and interpretation"
  },
  {
    "objectID": "blogs/gwas.html#what-youve-received",
    "href": "blogs/gwas.html#what-youve-received",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "What You’ve Received",
    "text": "What You’ve Received\nBefore diving into the analysis, ensure you have:\n\nGenotype data (after basic QC)\nPhenotype file\nCovariates including PC1–PC20\nRelated individuals in the cohort"
  },
  {
    "objectID": "blogs/gwas.html#post-qc-checks",
    "href": "blogs/gwas.html#post-qc-checks",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Post-QC Checks",
    "text": "Post-QC Checks\n\n1. Sample Matching\nEnsure sample IDs match across genotype and phenotype files.\ncomm -12 &lt;(cut -f1 phenotype.txt | sort) &lt;(cut -f1 genotype.fam | sort) &gt; common_ids.txt\n\n\n2. Sex Check\ncomm -12 &lt;(cut -f1 phenotype.txt | sort) &lt;(cut -f1 genotype.fam | sort) &gt; common_ids.txt\n\n\n3. Population Stratification\nYou already have PC1–PC20, so no need to compute PCA again. Just include these as covariates in your model.\n\n\n4. Relatedness\nSince your cohort includes related individuals, do not remove them. Instead, use a mixed model approach like SAIGE to account for relatedness."
  },
  {
    "objectID": "blogs/gwas.html#gwas-with-saige",
    "href": "blogs/gwas.html#gwas-with-saige",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "GWAS with SAIGE",
    "text": "GWAS with SAIGE\n\nWhy Choose SAIGE?\nSAIGE is particularly well-suited for complex datasets because it:\n\nHandles case-control imbalance effectively\nAccounts for relatedness using a sparse GRM\nScales to large datasets efficiently\nSupports both binary and quantitative traits\n\n\n\nConducting GWAS with SAIGE\nYou can follow the comprehensive guide to perform GWAS from their well-documented page."
  },
  {
    "objectID": "blogs/gwas.html#alternatives-to-saige",
    "href": "blogs/gwas.html#alternatives-to-saige",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Alternatives to SAIGE",
    "text": "Alternatives to SAIGE\nDepending on your specific needs, consider these alternatives:\n\n\n\n\n\n\n\n\nTool\nBest For\nNotes\n\n\n\n\nPLINK\nSmall datasets, basic GWAS\nFast, but doesn’t handle relatedness\n\n\nGEMMA\nMixed models, quantitative traits\nHandles relatedness well\n\n\nBOLT-LMM\nLarge-scale quantitative traits\nFast, limited binary trait support\n\n\nREGENIE\nBinary and quantitative traits\nScalable, supports stepwise modeling"
  },
  {
    "objectID": "blogs/gwas.html#output-expectations",
    "href": "blogs/gwas.html#output-expectations",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Output Expectations",
    "text": "Output Expectations\nAfter running your GWAS, you should expect:\n\nGWAS results: SNP ID, effect size, SE, p-value\nManhattan plot and QQ plot for visualization\nAdditional visualizations: Regional plots, volcano plots, etc.\n\n\nCreating Manhattan Plots\nThere are several online resources that input summary statistics and output Manhattan plots.\nPersonally, I opted for an in-house developed modified version of an R package used by research groups in the department.\nlibrary(qqman)\n\ngwas &lt;- read.table(\"gwas_results.txt\", header = TRUE) \n\nmanhattan(gwas, chr=\"CHR\", bp=\"BP\", snp=\"SNP\", p=\"P\", main=\"GWAS Manhattan Plot\") \n\nqq(gwas$P)\nThe output could look similar to the following plots:\n\n\n\n\n\n\n\nFigure 1: Manhattan Plot with red color indicating previously known loci and blue are the novel loci\n\n\n\n\n\n\n\n\n\n\nFigure 2: QQ Plot with a genomic correction value of 1.14\n\n\n\n\n\n\nNote: Taken from publication."
  },
  {
    "objectID": "blogs/gwas.html#downstream-checks-in-gwas",
    "href": "blogs/gwas.html#downstream-checks-in-gwas",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Downstream Checks in GWAS",
    "text": "Downstream Checks in GWAS\nOnce you’ve run your GWAS and identified significant associations, it’s important to assess the overall genetic architecture of the trait. Two key downstream checks are heritability estimation and LD Score Regression (LDSC).\n\nSNP-Based Heritability\nHeritability refers to the proportion of phenotypic variance in a trait that can be attributed to genetic variation. In GWAS, we often estimate SNP-based heritability, which considers only the common variants captured in the study.\n\nWhy it matters: It helps quantify how much of the trait is explained by the genotyped SNPs.\nTools: Common tools include GCTA, LDSC, and REML.\nExample: If SNP-based heritability is 0.25, it means 25% of the trait variance is explained by the SNPs used in the GWAS.\n\n\n\nLD Score Regression (LDSC)\nLD Score Regression is a method to distinguish true polygenic signal from confounding biases (like population stratification or cryptic relatedness).\n\nLD Score: A measure of how much a SNP tags nearby variants due to linkage disequilibrium (LD).\nKey Insight: If a trait is polygenic, SNPs with higher LD scores should have higher test statistics.\nIntercept: LDSC provides an intercept value that helps detect inflation due to confounding. An intercept close to 1 suggests minimal confounding.\nApplications:\n\nEstimating SNP heritability from summary statistics.\nPartitioning heritability across functional annotations.\nGenetic correlation between traits.\n\n\n\n\nLDSC Command\nldsc.py \\\n  --h2 sumstats.txt \\\n  --ref-ld-chr eur_w_ld_chr/ \\\n  --w-ld-chr eur_w_ld_chr/ \\\n  --out trait_heritability\n\n\nInterpretation\nLDSC helps you understand whether your GWAS results reflect true polygenic signal or are inflated due to confounding. Here’s how to interpret the main components:\n1. Heritability Estimate (\\(h^2\\))\nWhat it means: The proportion of phenotypic variance explained by all SNPs in your GWAS.\nInterpretation:\n\n\n\n\n\n\n\nHeritability Value\nMeaning\n\n\n\n\n&gt; 0.2\nStrong polygenic contribution\n\n\n&lt; 0.2\nLimited genetic influence or insufficient power\n\n\n\nNote: This is SNP-based heritability, not total heritability (which includes rare variants and other sources).\n\n2. Intercept\nWhat it means: Measures inflation in test statistics not due to polygenicity.\nIdeal value: Close to 1.0.\nInterpretation:\n\n\n\n\n\n\n\nIntercept Value\nMeaning\n\n\n\n\n≈ 1.0\nMinimal confounding; inflation is likely due to true polygenic signal.\n\n\n&gt; 1.0\nSuggests confounding (e.g., population stratification, cryptic relatedness).\n\n\n\nAction: If high, consider improving QC (e.g., more principal components, better ancestry matching).\n3. LDSC Ratio\nThe LDSC ratio is calculated as:\n\\[\n\\text{Ratio} = \\frac{\\text{Intercept} - 1}{\\text{Mean}(\\chi^2) - 1}\n\\]\nInterpretation:\n\n\n\n\n\n\n\n\nRatio Value\nMeaning\nAction\n\n\n\n\n&lt; 0.1\nMost inflation is due to true polygenic signal.\n✅ Your GWAS is likely well-controlled.\n\n\n0.1 – 0.3\nSome inflation may be due to confounding.\n⚠️ Consider checking population structure and relatedness.\n\n\n&gt; 0.3\nA large portion of inflation is likely due to confounding.\n❌ Revisit QC steps, include more PCs, or refine your model.\n\n\n\n 4. Test Statistic vs LD Score Plot\n\n\n\n\n\n\n\n\nTrend Type\nPlot\nInterpretation\n\n\n\n\nUpward Trend\n\nIndicates polygenic signal — SNPs with higher LD scores have higher test statistics.\n\n\nFlat/Noisy Trend\n\nSuggests lack of polygenicity or presence of confounding factors.\n\n\n\n\n\nSummary of LD Score Regression Metrics\n\n\n\n\n\n\n\n\n\nMetric\nIdeal Value\nWhat It Means\nInterpretation & Action\n\n\n\n\nHeritability (\\(h^2\\))\n&gt; 0.1\nProportion of trait variance explained by SNPs\nLow value → trait may be weakly polygenic or underpowered study\n\n\nIntercept\n≈ 1.0\nInflation in test statistics not due to polygenicity\n&gt;1.0 → possible confounding (e.g., population stratification)\n\n\nRatio\n&lt; 0.1\nFraction of inflation due to confounding\n&gt;0.3 → revisit QC, ancestry correction, or model assumptions\n\n\nLD Score Plot\nPositive slope\nRelationship between LD score and test statistic\nFlat/noisy → weak polygenic signal or confounding present"
  },
  {
    "objectID": "blogs/gwas.html#ldsc-command",
    "href": "blogs/gwas.html#ldsc-command",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "LDSC Command",
    "text": "LDSC Command\nldsc.py \\\n  --h2 sumstats.txt \\\n  --ref-ld-chr eur_w_ld_chr/ \\\n  --w-ld-chr eur_w_ld_chr/ \\\n  --out trait_heritability\n\nInterpretation\nHeritability estimate: Found in the .log file output.\nIntercept: If significantly &gt;1, consider revisiting population structure correction (e.g., using more PCs or better QC)."
  },
  {
    "objectID": "blogs/gwas.html#downstream-analysis-in-gwas",
    "href": "blogs/gwas.html#downstream-analysis-in-gwas",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Downstream Analysis in GWAS",
    "text": "Downstream Analysis in GWAS\nOnce you’ve run your GWAS and identified significant associations, it’s important to assess the overall genetic architecture of the trait. Two key downstream analyses are heritability estimation and LD Score Regression (LDSC).\n\nSNP-Based Heritability\nHeritability refers to the proportion of phenotypic variance in a trait that can be attributed to genetic variation. In GWAS, we often estimate SNP-based heritability, which considers only the common variants captured in the study.\n\nWhy it matters: It helps quantify how much of the trait is explained by the genotyped SNPs\nTools: Common tools include GCTA, LDSC, and REML\nExample: If SNP-based heritability is 0.25, it means 25% of the trait variance is explained by the SNPs used in the GWAS\n\n\n\nLD Score Regression (LDSC)\nLD Score Regression is a method to distinguish true polygenic signal from confounding biases (like population stratification or cryptic relatedness).\n\nLD Score: A measure of how much a SNP tags nearby variants due to linkage disequilibrium (LD)\nKey Insight: If a trait is polygenic, SNPs with higher LD scores should have higher test statistics\nIntercept: LDSC provides an intercept value that helps detect inflation due to confounding. An intercept close to 1 suggests minimal confounding\n\n\nLDSC Applications\n\nEstimating SNP heritability from summary statistics\nPartitioning heritability across functional annotations\nCalculating genetic correlation between traits\n\n\n\nLDSC Command\nldsc.py \\\n  --h2 sumstats.txt \\\n  --ref-ld-chr eur_w_ld_chr/ \\\n  --w-ld-chr eur_w_ld_chr/ \\\n  --out trait_heritability"
  },
  {
    "objectID": "blogs/gwas.html#interpreting-ldsc-results",
    "href": "blogs/gwas.html#interpreting-ldsc-results",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Interpreting LDSC Results",
    "text": "Interpreting LDSC Results\nLDSC helps you understand whether your GWAS results reflect true polygenic signal or are inflated due to confounding. Here’s how to interpret the main components:\n\n1. Heritability Estimate (h²)\nWhat it means: The proportion of phenotypic variance explained by all SNPs in your GWAS.\n\n\n\n\n\n\n\nHeritability Value\nMeaning\n\n\n\n\n&gt; 0.2\nStrong polygenic contribution\n\n\n&lt; 0.2\nLimited genetic influence or insufficient power\n\n\n\n\nNote: This is SNP-based heritability, not total heritability (which includes rare variants and other sources).\n\n\n\n2. Intercept\nWhat it means: Measures inflation in test statistics not due to polygenicity.\nIdeal value: Close to 1.0\n\n\n\n\n\n\n\nIntercept Value\nMeaning\n\n\n\n\n≈ 1.0\nMinimal confounding; inflation is likely due to true polygenic signal\n\n\n&gt; 1.0\nSuggests confounding (e.g., population stratification, cryptic relatedness)\n\n\n\nAction: If high, consider improving QC (e.g., more principal components, better ancestry matching).\n\n\n3. LDSC Ratio\nThe LDSC ratio is calculated as:\n\\[\n\\text{Ratio} = \\frac{\\text{Intercept} - 1}{\\text{Mean}(\\chi^2) - 1}\n\\]\n\n\n\n\n\n\n\n\nRatio Value\nMeaning\nAction\n\n\n\n\n&lt; 0.1\nMost inflation is due to true polygenic signal\n✅ Your GWAS is likely well-controlled\n\n\n0.1 – 0.3\nSome inflation may be due to confounding\n⚠️ Consider checking population structure and relatedness\n\n\n&gt; 0.3\nA large portion of inflation is likely due to confounding\n❌ Revisit QC steps, include more PCs, or refine your model\n\n\n\n\n\n4. Test Statistic vs LD Score Plot\n\n\n\n\n\n\n\n\nTrend Type\nPlot\nInterpretation\n\n\n\n\nUpward Trend\n\nIndicates polygenic signal — SNPs with higher LD scores have higher test statistics\n\n\nFlat/Noisy Trend\n\nSuggests lack of polygenicity or presence of confounding factors"
  },
  {
    "objectID": "blogs/gwas.html#summary-ld-score-regression-metrics",
    "href": "blogs/gwas.html#summary-ld-score-regression-metrics",
    "title": "Beyond QC: A Practical Guide to GWAS with SAIGE",
    "section": "Summary: LD Score Regression Metrics",
    "text": "Summary: LD Score Regression Metrics\n\n\n\n\n\n\n\n\n\nMetric\nIdeal Value\nWhat It Means\nInterpretation & Action\n\n\n\n\nHeritability (h²)\n&gt; 0.1\nProportion of trait variance explained by SNPs\nLow value → trait may be weakly polygenic or underpowered study\n\n\nIntercept\n≈ 1.0\nInflation in test statistics not due to polygenicity\n&gt;1.0 → possible confounding (e.g., population stratification)\n\n\nRatio\n&lt; 0.1\nFraction of inflation due to confounding\n&gt;0.3 → revisit QC, ancestry correction, or model assumptions\n\n\nLD Score Plot\nPositive slope\nRelationship between LD score and test statistic\nFlat/noisy → weak polygenic signal or confounding present"
  },
  {
    "objectID": "blogs/prs.html",
    "href": "blogs/prs.html",
    "title": "Polygenic Risk Score",
    "section": "",
    "text": "Polygenic Risk Scores (PRS) represent one of the most promising applications of genomics in personalized medicine and population health research. By aggregating the effects of thousands or millions of genetic variants across the genome, PRS provide a single metric that estimates an individual’s genetic predisposition to a particular trait or disease.\nThe field of PRS research has evolved rapidly, with two distinct but complementary approaches emerging:\n\nReproducing previously validated PRS - applying established scores to new populations or datasets\nDeveloping novel PRS methods - creating improved algorithms that enhance predictive accuracy\n\nThis comprehensive guide will explore both approaches, examining contemporary methodologies, practical implementation strategies, and the critical steps needed to successfully deploy PRS in research and clinical settings."
  },
  {
    "objectID": "blogs/prs.html#conceptual-foundation",
    "href": "blogs/prs.html#conceptual-foundation",
    "title": "Polygenic Risk Score",
    "section": "Conceptual Foundation",
    "text": "Conceptual Foundation\nA polygenic risk score is mathematically defined as:\n\\[PRS_i = \\sum_{j=1}^{m} \\beta_j \\cdot G_{ij}\\]\nWhere: - \\(PRS_i\\) is the polygenic risk score for individual \\(i\\) - \\(\\beta_j\\) is the effect size (weight) for SNP \\(j\\) - \\(G_{ij}\\) is the genotype dosage for individual \\(i\\) at SNP \\(j\\) - \\(m\\) is the total number of SNPs included in the score"
  },
  {
    "objectID": "blogs/prs.html#the-two-research-paradigms",
    "href": "blogs/prs.html#the-two-research-paradigms",
    "title": "Polygenic Risk Score",
    "section": "The Two Research Paradigms",
    "text": "The Two Research Paradigms\n\nReproducing Previously Validated PRS\nThis approach involves taking established PRS weights from published studies or databases and applying them to new datasets. The advantages include:\n\nSpeed and efficiency: No need for extensive method development\nEstablished validity: Scores have been validated in independent populations\nStandardization: Enables comparison across studies and populations\nClinical readiness: Many validated PRS are closer to clinical implementation\n\nKey considerations:\n\nPopulation ancestry matching\nSNP availability and imputation quality\nStrand alignment and allele matching\nQuality control procedures\n\n\n\nDeveloping Novel PRS Methods\nThis approach focuses on creating new algorithms and methodologies to improve predictive performance. Areas of active development include:\n\nAdvanced statistical methods: Bayesian approaches, machine learning\nMulti-ancestry modeling: Methods that work across diverse populations\nFunctional annotation integration: Incorporating biological knowledge\nCross-trait analysis: Leveraging pleiotropy between traits"
  },
  {
    "objectID": "blogs/prs.html#classical-approaches",
    "href": "blogs/prs.html#classical-approaches",
    "title": "Polygenic Risk Score",
    "section": "Classical Approaches",
    "text": "Classical Approaches\n\nClumping and Thresholding (C+T)\nThe traditional approach involves:\n\nClumping: Removing SNPs in linkage disequilibrium\nThresholding: Selecting SNPs based on p-value cutoffs\nScoring: Applying weights to selected variants\n\n# PLINK clumping example\nplink --bfile target_data \\\n      --clump gwas_summary.txt \\\n      --clump-p1 5e-8 \\\n      --clump-p2 0.01 \\\n      --clump-r2 0.1 \\\n      --clump-kb 250 \\\n      --out clumped_snps\n\n\nSimple Summing\nBasic additive model without sophisticated weighting:\n# Simple PRS calculation\nprs_simple &lt;- rowSums(genotype_matrix * effect_sizes, na.rm = TRUE)"
  },
  {
    "objectID": "blogs/prs.html#advanced-contemporary-methods",
    "href": "blogs/prs.html#advanced-contemporary-methods",
    "title": "Polygenic Risk Score",
    "section": "Advanced Contemporary Methods",
    "text": "Advanced Contemporary Methods\n\nLDpred and LDpred2\nLDpred represents a significant advancement by modeling linkage disequilibrium (LD) structure explicitly:\nKey innovations:\n\nBayesian framework incorporating LD information\nImproved handling of correlated variants\nBetter calibration of effect sizes\n\nLDpred2 improvements:\n\nComputational efficiency for biobank-scale data\nRobust handling of population stratification\nIntegration with sparse LD matrices\n\n# LDpred2 example workflow\nlibrary(bigsnpr)\n\n# Compute LD matrix\ncorr &lt;- snp_cor(G, ind.col = ind_val, ncores = nb_cores())\n\n# Run LDpred2-auto\nmulti_auto &lt;- snp_ldpred2_auto(corr, df_beta, h2_init = h2_est,\n                               vec_p_init = seq_log(1e-4, 0.9, 30),\n                               ncores = nb_cores())\n\n\nPRSice-2\nAn integrated tool providing:\n\nAutomated QC procedures\nMultiple P-value thresholds testing\nBuilt-in visualization capabilities\nCross-validation frameworks\n\n\n\nBayesian Methods\nSBayesR: Incorporates functional annotations and LD structure\nPRS-CS: Uses continuous shrinkage priors for effect size estimation\nBOLT-LMM: Linear mixed models for improved association testing"
  },
  {
    "objectID": "blogs/prs.html#method-comparisons-and-improvements",
    "href": "blogs/prs.html#method-comparisons-and-improvements",
    "title": "Polygenic Risk Score",
    "section": "Method Comparisons and Improvements",
    "text": "Method Comparisons and Improvements\n\nPerformance Metrics\nContemporary methods are evaluated using:\n\nArea Under Curve (AUC): Discriminative ability\nNagelkerke R²: Variance explained\nCalibration metrics: Agreement between predicted and observed risks\nNet Reclassification Index (NRI): Clinical utility assessment\n\n\n\nMethodological Evolution\nEach new method builds upon previous limitations:\n\nC+T → LDpred: Better LD modeling\nLDpred → LDpred2: Computational scalability\nSingle-ancestry → Multi-ancestry: Population generalizability\nLinear → Non-linear: Capturing epistatic interactions\nSNP-based → Annotation-based: Biological interpretability"
  },
  {
    "objectID": "blogs/prs.html#data-preparation-pipeline",
    "href": "blogs/prs.html#data-preparation-pipeline",
    "title": "Polygenic Risk Score",
    "section": "Data Preparation Pipeline",
    "text": "Data Preparation Pipeline\n\n1. Obtaining Reference PRS Weights\nSources for validated PRS include:\n\nPGS Catalog: Comprehensive repository of published scores\nOriginal publications: Supplementary materials\nBiobank repositories: UK Biobank, FinnGen, etc.\n\n# Download from PGS Catalog\nwget https://www.pgscatalog.org/rest/score/PGS000001/scoring_file/\n\n\n2. Target Dataset Preparation\n\nQuality Control Steps\n# Basic QC pipeline in PLINK\nplink --bfile raw_data \\\n      --maf 0.01 \\\n      --geno 0.05 \\\n      --mind 0.05 \\\n      --hwe 1e-6 \\\n      --make-bed \\\n      --out qc_data\n\n\nPopulation Stratification\n# Principal component analysis\nplink --bfile qc_data \\\n      --pca 10 \\\n      --out population_pcs\n\n\n\n3. Harmonization Process\nCritical steps for ensuring compatibility:\n\nSNP Matching and Alignment\n# R code for harmonization\nlibrary(data.table)\n\n# Load PRS weights\nprs_weights &lt;- fread(\"prs_weights.txt\")\nsetnames(prs_weights, c(\"SNP\", \"A1\", \"A2\", \"BETA\"))\n\n# Load target SNP info\ntarget_snps &lt;- fread(\"target_data.bim\")\nsetnames(target_snps, paste0(\"V\", 1:6), \n         c(\"CHR\", \"SNP\", \"CM\", \"BP\", \"A1\", \"A2\"))\n\n# Match SNPs\nmatched_snps &lt;- merge(prs_weights, target_snps, by = \"SNP\")\n\n# Handle strand flips and allele switches\nmatched_snps[, aligned_beta := ifelse(\n  (A1.x == A1.y & A2.x == A2.y) | \n  (A1.x == A2.y & A2.x == A1.y), \n  ifelse(A1.x == A1.y, BETA, -BETA), \n  NA\n)]\n\n# Remove ambiguous SNPs (A/T, G/C)\nambiguous &lt;- c(\"AT\", \"TA\", \"GC\", \"CG\")\nmatched_snps &lt;- matched_snps[\n  !paste0(A1.x, A2.x) %in% ambiguous\n]\n\n\nFrequency Alignment\n# Check allele frequency concordance\nfreq_check &lt;- fread(\"target_data.frq\")\nprs_freq &lt;- merge(matched_snps, freq_check, by = \"SNP\")\n\n# Flag discordant frequencies (&gt;0.2 difference)\nprs_freq[, freq_diff := abs(MAF - reference_freq)]\nproblematic_snps &lt;- prs_freq[freq_diff &gt; 0.2, SNP]"
  },
  {
    "objectID": "blogs/prs.html#prs-calculation-with-plink",
    "href": "blogs/prs.html#prs-calculation-with-plink",
    "title": "Polygenic Risk Score",
    "section": "PRS Calculation with PLINK",
    "text": "PRS Calculation with PLINK\n\nCreating Score Files\n# Prepare PLINK score file\nscore_file &lt;- matched_snps[!is.na(aligned_beta), \n                          .(SNP, A1.x, aligned_beta)]\nfwrite(score_file, \"prs_score.txt\", sep = \"\\t\", col.names = FALSE)\n\n\nRunning PLINK Score Calculation\n# Calculate PRS using PLINK\nplink --bfile qc_data \\\n      --score prs_score.txt \\\n      --out prs_results"
  },
  {
    "objectID": "blogs/prs.html#integration-with-individual-level-data",
    "href": "blogs/prs.html#integration-with-individual-level-data",
    "title": "Polygenic Risk Score",
    "section": "Integration with Individual-Level Data",
    "text": "Integration with Individual-Level Data\n\nLoading and Merging Data\n# Load PLINK results\nprs_results &lt;- fread(\"prs_results.sscore\")\nsetnames(prs_results, c(\"FID\", \"IID\", \"ALLELE_CT\", \"NAMED_ALLELE_CT\", \"SCORE1_AVG\"))\n\n# Load individual-level data\nindividual_data &lt;- fread(\"individual_phenotypes.txt\")\n\n# Merge datasets\nfinal_data &lt;- merge(individual_data, \n                   prs_results[, .(IID, PRS = SCORE1_AVG)], \n                   by = \"IID\")"
  },
  {
    "objectID": "blogs/prs.html#normalization-and-standardization",
    "href": "blogs/prs.html#normalization-and-standardization",
    "title": "Polygenic Risk Score",
    "section": "Normalization and Standardization",
    "text": "Normalization and Standardization\n\nRank-Based Inverse Normal Transformation\nThe rank-based inverse normal transformation ensures interpretability and removes distributional assumptions:\n# Rank-based inverse normal transformation function\ninverse_normal_transform &lt;- function(x) {\n  # Remove missing values\n  non_missing &lt;- !is.na(x)\n  x_clean &lt;- x[non_missing]\n  \n  # Calculate ranks\n  ranks &lt;- rank(x_clean, ties.method = \"average\")\n  \n  # Convert to quantiles (0, 1)\n  quantiles &lt;- (ranks - 0.5) / length(ranks)\n  \n  # Apply inverse normal transformation\n  transformed &lt;- qnorm(quantiles)\n  \n  # Create output vector\n  result &lt;- rep(NA, length(x))\n  result[non_missing] &lt;- transformed\n  \n  return(result)\n}\n\n# Apply transformation\nfinal_data[, PRS_normalized := inverse_normal_transform(PRS)]"
  },
  {
    "objectID": "blogs/prs.html#validation-and-quality-assessment",
    "href": "blogs/prs.html#validation-and-quality-assessment",
    "title": "Polygenic Risk Score",
    "section": "Validation and Quality Assessment",
    "text": "Validation and Quality Assessment\n\nDistribution Checks\nlibrary(ggplot2)\n\n# Plot distributions\np1 &lt;- ggplot(final_data, aes(x = PRS)) +\n  geom_histogram(bins = 50, alpha = 0.7, fill = \"blue\") +\n  labs(title = \"Raw PRS Distribution\")\n\np2 &lt;- ggplot(final_data, aes(x = PRS_normalized)) +\n  geom_histogram(bins = 50, alpha = 0.7, fill = \"red\") +\n  labs(title = \"Normalized PRS Distribution\")\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\nPerformance Validation\n# Association testing (example for binary trait)\nif(\"case_status\" %in% names(final_data)) {\n  # Logistic regression\n  model &lt;- glm(case_status ~ PRS_normalized + PC1 + PC2 + PC3 + age + sex,\n               data = final_data, family = binomial)\n  \n  # Extract results\n  prs_or &lt;- exp(coef(model)[\"PRS_normalized\"])\n  prs_pvalue &lt;- summary(model)$coefficients[\"PRS_normalized\", \"Pr(&gt;|z|)\"]\n  \n  # Calculate AUC\n  library(pROC)\n  roc_result &lt;- roc(final_data$case_status, \n                   predict(model, type = \"response\"))\n  auc_value &lt;- auc(roc_result)\n  \n  cat(\"PRS Odds Ratio:\", round(prs_or, 3), \"\\n\")\n  cat(\"P-value:\", format(prs_pvalue, scientific = TRUE), \"\\n\")\n  cat(\"AUC:\", round(auc_value, 3), \"\\n\")\n}"
  },
  {
    "objectID": "blogs/prs.html#population-ancestry-and-transferability",
    "href": "blogs/prs.html#population-ancestry-and-transferability",
    "title": "Polygenic Risk Score",
    "section": "Population Ancestry and Transferability",
    "text": "Population Ancestry and Transferability\n\nAncestry-Specific Considerations\n# Check ancestry composition\nif(\"ancestry\" %in% names(final_data)) {\n  # Performance by ancestry\n  performance_by_ancestry &lt;- final_data[, .(\n    mean_prs = mean(PRS_normalized, na.rm = TRUE),\n    sd_prs = sd(PRS_normalized, na.rm = TRUE),\n    n = .N\n  ), by = ancestry]\n  \n  print(performance_by_ancestry)\n}\n\n\nMulti-Ancestry Adjustments\n# Ancestry-specific normalization\nfinal_data[, PRS_ancestry_norm := inverse_normal_transform(PRS), by = ancestry]"
  },
  {
    "objectID": "blogs/prs.html#clinical-implementation-considerations",
    "href": "blogs/prs.html#clinical-implementation-considerations",
    "title": "Polygenic Risk Score",
    "section": "Clinical Implementation Considerations",
    "text": "Clinical Implementation Considerations\n\nRisk Stratification\n# Create risk categories\nfinal_data[, risk_category := cut(\n  PRS_percentile,\n  breaks = c(0, 20, 40, 60, 80, 100),\n  labels = c(\"Very Low\", \"Low\", \"Average\", \"High\", \"Very High\"),\n  include.lowest = TRUE\n)]\n\n# Risk category summary\nrisk_summary &lt;- final_data[, .(\n  n = .N,\n  case_rate = mean(case_status, na.rm = TRUE)\n), by = risk_category]\n\n\nReporting and Interpretation\n# Generate individual reports\ngenerate_prs_report &lt;- function(individual_id, final_data) {\n  ind_data &lt;- final_data[IID == individual_id]\n  \n  cat(\"=== PRS Report for Individual\", individual_id, \"===\\n\")\n  cat(\"Raw PRS:\", round(ind_data$PRS, 4), \"\\n\")\n  cat(\"Percentile:\", round(ind_data$PRS_percentile, 1), \"%\\n\")\n  cat(\"Risk Category:\", as.character(ind_data$risk_category), \"\\n\")\n  cat(\"Normalized Score:\", round(ind_data$PRS_normalized, 3), \"\\n\")\n}"
  },
  {
    "objectID": "blogs/prs.html#quality-control-checklist",
    "href": "blogs/prs.html#quality-control-checklist",
    "title": "Polygenic Risk Score",
    "section": "Quality Control Checklist",
    "text": "Quality Control Checklist\n\nPre-calculation QC:\n\nSNP call rate &gt; 95%\nIndividual call rate &gt; 95%\nHardy-Weinberg equilibrium p &gt; 1e-6\nMinor allele frequency &gt; 1%\n\nHarmonization QC:\n\nStrand alignment verification\nAllele frequency concordance\nPopulation ancestry matching\nSNP position consistency\n\nPost-calculation QC:\n\nDistribution normality assessment\nOutlier identification\nPopulation stratification testing\nPerformance validation"
  },
  {
    "objectID": "blogs/prs.html#documentation-and-reproducibility",
    "href": "blogs/prs.html#documentation-and-reproducibility",
    "title": "Polygenic Risk Score",
    "section": "Documentation and Reproducibility",
    "text": "Documentation and Reproducibility\n# Session information\nsessionInfo()\n\n# Parameter documentation\nprs_params &lt;- list(\n  prs_source = \"PGS Catalog ID: PGS000001\",\n  target_population = \"European ancestry\",\n  qc_filters = list(\n    maf = 0.01,\n    geno = 0.05,\n    mind = 0.05,\n    hwe = 1e-6\n  ),\n  n_snps_original = nrow(prs_weights),\n  n_snps_matched = nrow(matched_snps),\n  n_individuals = nrow(final_data)\n)\n\nsaveRDS(prs_params, \"prs_calculation_parameters.rds\")"
  },
  {
    "objectID": "blogs/prs.html#emerging-methodologies",
    "href": "blogs/prs.html#emerging-methodologies",
    "title": "Polygenic Risk Score",
    "section": "Emerging Methodologies",
    "text": "Emerging Methodologies\n\nMulti-omic integration: Incorporating transcriptomic and epigenomic data\nDynamic PRS: Age and environment-specific risk scores\n\nPathway-based PRS: Biologically-informed scoring systems\nFederated learning: Privacy-preserving collaborative model development"
  },
  {
    "objectID": "blogs/prs.html#clinical-translation-challenges",
    "href": "blogs/prs.html#clinical-translation-challenges",
    "title": "Polygenic Risk Score",
    "section": "Clinical Translation Challenges",
    "text": "Clinical Translation Challenges\n\nRegulatory approval processes\nHealthcare system integration\nProvider education and training\nPatient communication strategies\nHealth equity considerations"
  },
  {
    "objectID": "blogs/prs.html#code-availability",
    "href": "blogs/prs.html#code-availability",
    "title": "Polygenic Risk Score",
    "section": "Code Availability",
    "text": "Code Availability\nAll code examples in this post are available in the accompanying GitHub repository: https://github.com/username/prs-tutorial"
  },
  {
    "objectID": "blogs/prs.html#acknowledgments",
    "href": "blogs/prs.html#acknowledgments",
    "title": "Polygenic Risk Score",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nWe thank the global genomics community for developing and sharing the tools and methods that make PRS research possible."
  },
  {
    "objectID": "blogs/mcmc.html",
    "href": "blogs/mcmc.html",
    "title": "Monte Carlo Markov Chain (MCMC)",
    "section": "",
    "text": "Coming soon!!!\n\n\n\n Back to top"
  },
  {
    "objectID": "blogs/healthecon.html",
    "href": "blogs/healthecon.html",
    "title": "Health Economics and R",
    "section": "",
    "text": "Coming soon!!!\n\n\n\n Back to top"
  },
  {
    "objectID": "resume.html#key-qualifications",
    "href": "resume.html#key-qualifications",
    "title": "Curriculum Vitae",
    "section": "Key Qualifications",
    "text": "Key Qualifications\n\nPhD in Genetic Epidemiology, dual Master’s degrees in Biostatistics, and a Bachelor’s in Statistics, with over six years of experience in health-related data analysis.\nProficient in epidemiological and health economic research methods, including survival analysis, longitudinal modeling, and utility analyses, with practical expertise in R and Excel.\nSkilled at translating complex statistical concepts into digestible topics for diverse audiences, including internal teams and external stakeholders.\nHighly adaptable in fast-paced environments, capable of managing multiple roles, meeting tight deadlines, and making informed decisions under pressure.\nCollaborative team player who fosters a supportive and productive work atmosphere through effective communication and cooperation.\nStrong written and verbal communication skills in English, with experience in technical documentation and client-facing reporting."
  },
  {
    "objectID": "resume.html#skills-and-interests",
    "href": "resume.html#skills-and-interests",
    "title": "Curriculum Vitae",
    "section": "Skills and Interests",
    "text": "Skills and Interests\nSoftware: R, Stata, Python, SAS, Git, Bash, Excel (VBA)\nMethods: Parametric survival analysis, Longitudinal analysis, Evidence synthesis (network meta-analysis), Indirect treatment comparison (MAIC, PAIC, STC, ML-NMR), Utility analysis (cost-effectiveness, cost-utility, budget-impact)\nSoft Skills: Client-focused, Empathetic collaborator, Clear communicator, Flexible when required, Logical decision-maker\nHobbies: Hiking, Downhill skiing, Volunteering, Cooking, Learning Norwegian"
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Curriculum Vitae",
    "section": "Certifications",
    "text": "Certifications\n\nEconomic Evaluation Modelling in R, Bristol Medical School (14, 16 and 21 July 2025)\n\nAdvanced Multiple Imputation Methods to Deal with Missing Data, Bristol Medical School (5–6 December 2024)"
  }
]